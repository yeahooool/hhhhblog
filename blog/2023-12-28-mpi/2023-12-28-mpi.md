---
slug: mpi
title: MPI实现多进程
authors: [hf]
tags: [mpi, c++]
---

以下 C++ 代码通过加载 torchscript 模型，并调用模型进行预测。其中加载模型部分如下：

```cpp
void PairPWMLFF::settings(int narg, char** arg)
{
    int ff_idx;
    int iarg;

    if (narg <= 0) error->all(FLERR, "Illegal pair_style command");
    std::vector<std::string> models;

    while (iarg < narg) {
        iarg++;
    }
    num_ff = utils::inumeric(FLERR, arg[0], false, lmp);    // number of models
    for (int ii = 1; ii < iarg; ++ii) {
        models.push_back(arg[ii]);                          // model files
    }

    device = torch::cuda::is_available() ? torch::kCUDA : torch::kCPU;
    dtype = torch::kFloat64;
    if (me == 0) utils::logmesg(this -> lmp, "<---- Loading model ---->");
    for (ff_idx = 0; ff_idx < num_ff; ff_idx++) {
        std::string model_file = models[ff_idx];
        try
        {
            module = torch::jit::load(model_file, c10::Device(device));
            module.to(dtype);
            modules.push_back(module);
            if (me == 0) printf("\nLoading model file:   %s\n", model_file.c_str());
        }
        catch (const c10::Error e)
        {
            std::cerr << "Failed to load model :" << e.msg() << std::endl;
        }
    }
    cutoff = module.attr("Rmax").toDouble();
    max_neighbor = module.attr("maxNeighborNum").toInt();
    // print information
    if (me == 0) {
    utils::logmesg(this -> lmp, "<---- Load model successful!!! ---->");
    printf("\nDevice:       %s", device == torch::kCPU ? "CPU" : "GPU");
    printf("\nModel type:   %5d",5);
    printf("\nModel nums:   %5d",num_ff);
    printf("\ncutoff :      %12.6f",cutoff);
    printf("\nmax_neighbor: %5d\n", max_neighbor);
    }
}
```

已知所有的模型都被加载到同一个设备上，这是通过以下代码行实现的：

```cpp
module = torch::jit::load(model_file, c10::Device(device));
```

在这里，`device` 是一个变量，它被设置为 GPU（如果可用）或 CPU。然后，这个设备被用作加载模型的目标设备。

:::info
头文件中进行了相关声明及定义：

```cpp
torch::jit::script::Module module;
std::vector<torch::jit::script::Module> modules;
torch::Device device = torch::kCPU;
torch::Dtype dtype = torch::kFloat32;
```

:::

<!-- truncate -->

如果希望通过 MPI 实现多进程，并且每个模型进程都在不同的 GPU 上运行，需要在加载每个模型时指定不同的设备。那么可以通过 MPI 的进程 ID 来确定应该使用哪个 GPU。在 MPI 中，每个进程都有一个唯一的进程 ID，通常称为"`rank`"。使用这个 `rank` 来确定应该在哪个 GPU 上加载和运行模型，如下所示：

```cpp
void PairPWMLFF::settings(int narg, char** arg)
{
    int ff_idx;
    int iarg;
    int rank;
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);    // 获取MPI进程的rank

// ... other codes

    torch::DeviceType device_type = torch::cuda::is_available() ? torch::kCUDA : torch::kCPU;    // 判断设备类型
    device = device_type == torch::kCUDA ?  torch::Device(device_type, rank) : torch::Device(device_type);    // 如果设备类型是GPU，使用rank作为设备ID
    dtype = torch::kFloat64;
    if (me == 0) utils::logmesg(this -> lmp, "<---- Loading model ---->");
    for (ff_idx = 0; ff_idx < num_ff; ff_idx++) {
        std::string model_file = models[ff_idx];
        try
        {
            module = torch::jit::load(model_file, device);    // 加载模型到指定的设备
            module.to(dtype);
            modules.push_back(module);
            if (me == 0) printf("\nLoading model file:   %s\n", model_file.c_str());
        }
        catch (const c10::Error e)
        {
            std::cerr << "Failed to load model :" << e.msg() << std::endl;
        }
    }
// ... other codes
}
```

在这个修改后的代码中，我们首先获取当前进程的 MPI rank，然后判断设备类型。如果设备类型是 GPU，我们使用这个 `rank` 作为设备 ID 来定义一个新的 `torch::Device` 对象。如果设备类型是 CPU，我们直接使用 CPU 作为设备。然后，我们在这个设备上加载和运行模型。

:::caution
这个修改假设有足够多的 GPU 来运行所有的 MPI 进程。如果 GPU 数量少于 MPI 进程数量，那么需要添加一些额外的逻辑来处理这种情况。

可能会造成多个进程尝试在同一 GPU 上运行。这种情况下，可以在定义 `torch::Device` 对象时使用 MPI 的 `rank` 和 GPU 数量的模（`%`）作为设备 ID。这样，即使 MPI 进程数量大于 GPU 数量，每个进程也会被分配到一个有效的 GPU。

```cpp
// 获取MPI进程的rank和总进程数
int rank, size;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);
MPI_Comm_size(MPI_COMM_WORLD, &size);

// 获取GPU数量
int num_gpus = torch::cuda::device_count();

// 判断设备类型
torch::DeviceType device_type = torch::cuda::is_available() ? torch::kCUDA : torch::kCPU;

// 如果设备类型是GPU，使用rank和GPU数量的模作为设备ID
device = device_type == torch::kCUDA ? torch::Device(device_type, rank % num_gpus) : torch::Device(device_type);
```

:::

---

多线程运行模型时(`mpirun -np 4`)，`cuda`代码出现内存非法访问错误：

```python
# other codes

Force = CalcOps.calculateForce(list_neigh, dE, Ri_d, Force, nghost_tensor)[0]
Virial = CalcOps.calculateVirial(list_neigh, dE, ImageDR, Ri_d, nghost_tensor)[0]
```

其中`calculateForce`和`calculateVirial`包含部分`cuda`代码。

```bash
terminate called after throwing an instance of 'c10::Error' what(): CUDA error: an illegal memory access was encountered CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
...
```

这个错误通常表示 CUDA 内核试图访问无效的 GPU 内存地址。这可能是由于以下原因之一：

1. 试图访问已经被释放的 GPU 内存。
2. 试图访问超出 GPU 内存范围的地址。
3. 在一个 GPU 上分配了内存，但在另一个 GPU 上访问了这个内存。

对于第三种情况，需要确保在每个 MPI 进程中，所有的 CUDA 操作（包括内存分配和访问）都在正确的 GPU 上执行。

在多进程环境中，每个进程都需要有自己的 CUDA 上下文。CUDA 上下文是 CUDA 运行时系统中的一个重要概念，它包含了 CUDA 程序运行所需要的所有资源。一个 CUDA 上下文只能被一个 CPU 线程在一次运行中使用，而且一个 CPU 线程也只能使用一个 CUDA 上下文。

因此，如果代码使用了 CUDA，并且在多进程环境中运行代码，需要确保每个进程都有自己的 CUDA 上下文。如果试图在多个进程之间共享 CUDA 上下文，可能会导致问题。

**解决方法：**

可以在 CUDA 程序中使用 `cudaSetDevice` 函数来设置 CUDA 上下文。这个函数接受一个 GPU ID 作为参数，并将当前线程的 CUDA 上下文设置为这个 GPU。在多进程环境中，可以使用 MPI 的 `rank` 作为 GPU ID，这样每个进程都会在不同的 GPU 上运行。

例如：

```cpp
#include <mpi.h>
// 获取MPI进程的rank
int rank;
MPI_Comm_rank(MPI_COMM_WORLD, &rank);

// 设置CUDA上下文
cudaSetDevice(rank);
```

但是在前面加载模型的代码中，我们已经使用了 MPI 的 `rank` 来确定应该在哪个 GPU 上加载模型。因此，我们通可以过获取传入 CUDA 代码的变量的`device`到 CUDA 代码中，然后在 CUDA 代码中设置 CUDA 上下文。

```cpp
void torch_launch_calculate_force(torch::Tensor &nblist,
                       const torch::Tensor &dE,
                       const torch::Tensor &Ri_d,
                       int64_t batch_size,
                       int64_t natoms,
                       int64_t neigh_num,
                       const torch::Tensor &force,
                       int64_t nghost
)
{
    auto dtype = dE.dtype();
    assert(Ri_d.dtype() == dtype);
    assert(force.dtype() == dtype);

    // Get the device ID from the force tensor
    int device_id = force.device().index();

    if (dtype == torch::kFloat32)
    {
        launch_calculate_force<float>(
            (const int *) nblist.data_ptr(),
            (const float *) dE.data_ptr(),
            (const float *) Ri_d.data_ptr(),
            batch_size, natoms, neigh_num,
            (float *) force.data_ptr(),
            nghost,
            device_id  // Pass device_id to launch_calculate_force
        );
    } else if (dtype == torch::kFloat64)
    {
        launch_calculate_force<double>(
            (const int *) nblist.data_ptr(),
            (const double *) dE.data_ptr(),
            (const double *) Ri_d.data_ptr(),
            batch_size, natoms, neigh_num,
            (double *) force.data_ptr(),
            nghost,
            device_id  // Pass device_id to launch_calculate_force
        );
    }
}
```

`force.device().index()`是一个 PyTorch 的方法，用于获取 `force` 张量所在的设备 ID。

`force.device()`返回一个 `torch::Device` 对象，这个对象表示 `force` 张量所在的设备。`torch::Device` 对象有一个 `index()`方法，这个方法返回设备的索引。对于 CPU 设备，索引总是-1。对于 CUDA 设备，索引是设备的 ID，从 0 开始。

例如，如果 `force` 张量在第一个 CUDA 设备上，那么 `force.device().index()`将返回 0。如果 `force` 张量在第二个 CUDA 设备上，那么 `force.device().index()`将返回 1，以此类推。

然后，我们将这个设备 ID 传递给 CUDA 代码，通过`cudaSetDevice`函数设置 CUDA 代码进程所在的设备。

```cpp
template<typename DType>
void launch_calculate_force(
    const int * nblist,
    const DType * dE,
    const DType * Ri_d,
    const int batch_size,
    const int natoms,
    const int neigh_num,
    DType * force,
    const int nghost,
    const int device_id
) {

    cudaSetDevice(device_id);
    const int LEN = 256;
    const int nblock = (neigh_num + LEN - 1) / LEN;
    dim3 block_grid(nblock, natoms, batch_size);
    dim3 thread_grid(LEN, 3);
    force_calc<<<block_grid, thread_grid>>>(force, dE, Ri_d, nblist, natoms, neigh_num);
}

template void launch_calculate_force(const int * nblist,
    const float * dE,
    const float * Ri_d,
    const int batch_size,
    const int natoms,
    const int neigh_num,
    float * force,
    const int nghost,
    const int device_id);

template void launch_calculate_force(const int * nblist,
    const double * dE,
    const double * Ri_d,
    const int batch_size,
    const int natoms,
    const int neigh_num,
    double * force,
    const int nghost,
    const int device_id);
```
