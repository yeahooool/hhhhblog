---
slug: libtorch1
title: libtorch修改使用记录
authors: [hf]
tags: [python, libtorch]
---

使用 libtorch 的时候，发现有些地方和 pytorch 的使用方式不一样，记录一下。

TorchScript 有一些限制，特别是关于它如何处理和推断类型的。比如说以下函数传入的 `cfg` 字典包含了不同类型的值（`List[int]`、`bool` 和 `str`），这使得 TorchScript 难以处理。

```python
class EmbeddingNet(nn.Module):
    def __init__(self, cfg, magic=False):
        super(EmbeddingNet, self).__init__()
        self.cfg = cfg
        self.weights = nn.ParameterDict()

        if cfg['bias']:
            self.bias = nn.ParameterDict()
        if self.cfg['resnet_dt']:
                self.resnet_dt = nn.ParameterDict()
        self.network_size = [1] + self.cfg['network_size']
        if cfg["activation"] == "tanh":
            cfg["activation"] = torch.tanh
        else:
            pass
```

为了解决这个问题，我们可以在 `EmbeddingNet` 类中使用明确的类属性代替 `cfg` 字典，并为这些属性提供正确的类型注释。这有助于 TorchScript 推断这些属性的类型，并使代码与 TorchScript 更加兼容。以下是根据这种策略修改过的 `EmbeddingNet` 类：

<!-- truncate -->

```python
class EmbeddingNet(nn.Module):
    def __init__(self,
                 network_size: List[int],
                 bias: bool,
                 resnet_dt: bool,
                 activation: str,
                 device,
                 magic = False):
        super(EmbeddingNet, self).__init__()
        self.network_size = [1] + network_size
        self.bias_flag = bias
        self.resnet_dt_flag = resnet_dt
        self.activation = torch.tanh if activation == "tanh" else None

        self.weights = nn.ParameterDict()
        self.bias_dict = nn.ParameterDict() if bias else None
        self.resnet_dt_dict = nn.ParameterDict() if resnet_dt else None
```

这样做其实就是在调用 `EmbeddingNet` 的时候，把 `cfg` 字典中的值一个个明确地传入 `EmbeddingNet` 类中，这样 TorchScript 就能够推断出这些值的类型了。

```python
EmbeddingNet(self.config["net_cfg"]["embedding_net"]["network_size"],
             self.config["net_cfg"]["embedding_net"]["bias"],
             self.config["net_cfg"]["embedding_net"]["resnet_dt"],
             self.config["net_cfg"]["embedding_net"]["activation"],
             self.device,
             magic)
```

---

后续程序运行的时候，发现报错了，报错信息如下：

```python
RuntimeError:
Unable to extract string literal index. ModuleDict indexing is only supported with string literals. For example, 'i = "a"; self.layers[i](x)' will fail because i is not a literal. Enumeration of ModuleDict is supported, e.g. 'for k, v in self.items(): out = v(inp)':
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_embedding.py", line 89

            if self.bias_flag:
                hiden = torch.matmul(x, self.weights['weight' + str(i-1)]) + self.bias['bias' + str(i-1)]
                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
            else:
                hiden = torch.matmul(x, self.weights['weight' + str(i-1)])
```

这个新的错误表明 TorchScript 在处理字典键时遇到了问题。TorchScript 需要在编译时知道确切的键，但在代码中，键是通过字符串连接动态生成的，这使得 TorchScript 无法在编译时确定键的值。

由于 `nn.ParameterDict` 本质上是一个字典，并且在 forward 函数中使用的键是动态生成的，TorchScript 无法在编译时解析这些键。为了克服这个问题，我们可以尝试不使用 `nn.ParameterDict`，而是直接使用 `nn.Parameter` 列表或 `nn.ModuleList`

```python
self.weights = nn.ParameterDict()
self.bias_dict = nn.ParameterDict() if bias else None
self.resnet_dt_dict = nn.ParameterDict() if resnet_dt else None
# initial weight normalization
for i in range(1, len(self.network_size)):
    wij = torch.Tensor(self.network_size[i-1], self.network_size[i])
    normal(wij, mean=0, std=(1.0 / np.sqrt(self.network_size[i-1] + self.network_size[i])))
    self.weights["weight" + str(i-1)] = nn.Parameter(wij, requires_grad=True)
    if self.bias_flag:
        bias = torch.Tensor(1, self.network_size[i])
        normal(bias, mean=0, std=1)
        self.bias["bias" + str(i-1)] = nn.Parameter(bias, requires_grad=True)
    if self.resnet_dt_flag:
        resnet_dt = torch.Tensor(1, self.network_size[i])
        normal(resnet_dt, mean=1, std=0.001)
        self.resnet_dt["resnet_dt" + str(i-1)] = nn.Parameter(resnet_dt, requires_grad=True)

def forward(self, x):
    for i in range(1, len(self.network_size)):
        if self.bias_flag:
            hiden = torch.matmul(x, self.weights['weight' + str(i-1)]) + self.bias['bias' + str(i-1)]
        else:
            hiden = torch.matmul(x, self.weights['weight' + str(i-1)])

        hiden = self.activation(hiden)

        if self.network_size[i] == self.network_size[i-1]:
            if self.resnet_dt_flag:
                x = hiden * self.resnet_dt['resnet_dt' + str(i-1)] + x
            else:
                x = hiden + x
        elif self.network_size[i] == 2 * self.network_size[i-1]:
            if self.resnet_dt_flag:
                x = torch.cat((x, x), dim=-1)  + hiden * self.resnet_dt['resnet_dt' + str(i-1)]
            else:
                x = torch.cat((x, x), dim=-1)  + hiden
        else:
            x = hiden
    return x
```

以下是一个使用 `nn.Parameter` 列表的示例，它避免了在 forward 方法中使用动态键：

```python
# 初始化权重和偏差列表
self.weights = nn.ParameterList()
self.bias = nn.ParameterList() if bias else None
self.resnet_dt = nn.ParameterList() if resnet_dt else None

# 初始化权重和偏差
for i in range(1, len(self.network_size)):
    wij = torch.Tensor(self.network_size[i-1], self.network_size[i])
    normal(wij, mean=0, std=(1.0 / np.sqrt(self.network_size[i-1] + self.network_size[i])))
    self.weights.append(nn.Parameter(wij, requires_grad=True)).to('cuda:2')

    if self.bias_flag:
        bias_tensor = torch.Tensor(1, self.network_size[i])
        normal(bias_tensor, mean=0, std=1)
        self.bias.append(nn.Parameter(bias_tensor, requires_grad=True)).to('cuda:2')

    if self.resnet_dt_flag:
        resnet_dt = torch.Tensor(1, self.network_size[i])
        normal(resnet_dt, mean=1, std=0.001)
        self.resnet_dt.append(nn.Parameter(resnet_dt, requires_grad=True)).to('cuda:2')

def forward(self, x):
    for i in range(1, len(self.network_size)):
        weight = self.weights[i-1]
        if self.bias_flag:
            bias = self.bias[i-1]
            hiden = torch.matmul(x, weight) + bias
        else:
            hiden = torch.matmul(x, weight)

        hiden = self.activation_function(hiden)

        if self.network_size[i] == self.network_size[i-1]:
            if self.resnet_dt_flag:
                resnet_dt = elf.resnet_dt[i-1]
                x = hiden * resnet_dt + x
            else:
                x = hiden + x
        elif self.network_size[i] == 2 * self.network_size[i-1]:
            if self.resnet_dt_flag:
                resnet_dt = elf.resnet_dt[i-1]
                x = torch.cat((x, x), dim=-1) + hiden * resnet_dt
            else:
                x = torch.cat((x, x), dim=-1) + hiden
        else:
            x = hiden

    return x
```

---

后续程序运行的时候，发现仍然报错了，报错信息如下：

```python
RuntimeError:
Expected integer literal for index but got a variable or non-integer. ModuleList/Sequential indexing is only supported with integer literals. For example, 'i = 4; self.layers[i](x)' will fail because i is not a literal. Enumeration is supported, e.g. 'for index, v in enumerate(self): out = v(inp)':
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_embedding.py", line 93

            weight = self.weights[i-1]
                     ~~~~~~~~~~~~~~~~~ <--- HERE
            if self.bias_flag:
                bias = self.bias[i-1]
```

这个错误信息再次突出了 TorchScript 编译过程中的一些限制。TorchScript 期望在编译时能够知道所有的索引，但在代码中，试图使用变量 `i` 来索引 `nn.ParameterList`，这是 TorchScript 不支持的。

为了解决这个问题，我们可以用一个小技巧：使用 Python 内置的 `enumerate` 函数来遍历权重和偏置，这样我们可以同时获得当前的索引和对应的参数。

因为有多个需要遍历的参数列表（`self.weights`, `self.bias`, `self.resnet_dt`），而 TorchScript 不允许使用非文字索引（即变量索引）。

那么一种可能的解决方案是在初始化时将这些权重、偏置和 resnet_dt 参数组合成一个单独的模块列表 (`nn.ModuleList`)，每个模块都有它需要的参数。这样，可以一次遍历一个模块，并在每次迭代中应用逻辑。

下面是如何做到这一点的示例代码：

```python
class LayerModule(nn.Module):
    def __init__(self,
                 weight,
                 bias = None,
                 resnet_dt = None):
        super(LayerModule, self).__init__()
        self.weight = nn.Parameter(weight, requires_grad=True)
        self.bias = nn.Parameter(bias, requires_grad=True) if bias is not None else None
        self.resnet_dt = nn.Parameter(resnet_dt, requires_grad=True) if resnet_dt is not None else None

class EmbeddingNet(nn.Module):
    def __init__(self,
                 network_size: List[int],
                 bias: bool,
                 resnet_dt: bool,
                 activation: str,
                 device,
                 magic = False):
        super(EmbeddingNet, self).__init__()
        self.network_size = [1] + network_size
        self.bias_flag = bias
        self.resnet_dt_flag = resnet_dt
        self.activation = torch.tanh if activation == "tanh" else None
        # 创建模块列表
        self.layers = nn.ModuleList()

        # initial weight normalization
        for i in range(1, len(self.network_size)):
            wij = torch.Tensor(self.network_size[i-1], self.network_size[i])
            normal(wij, mean=0, std=(1.0 / np.sqrt(self.network_size[i-1] + self.network_size[i])))

            bias = None
            if self.bias_flag:
                bias = torch.Tensor(1, self.network_size[i])
                normal(bias, mean=0, std=1)

            resnet_dt = None
            if self.resnet_dt_flag:
                resnet_dt = torch.Tensor(1, self.network_size[i])
                normal(resnet_dt, mean=1, std=0.001)
            # 添加到模块列表
            self.layers.append(LayerModule(wij, bias, resnet_dt)).to(device)

    def forward(self, x):
        for i, layer in enumerate(self.layers):
            if self.bias_flag and layer.bias is not None:
                hiden = torch.matmul(x, layer.weight) + layer.bias
            else:
                hiden = torch.matmul(x, layer.weight)

            hiden = self.activation(hiden)

            if self.network_size[i+1] == self.network_size[i]:
                if self.resnet_dt_flag and layer.resnet_dt is not None:
                    x = hiden * layer.resnet_dt + x
                else:
                    x = hiden + x
            elif self.network_size[i+1] == 2 * self.network_size[i]:
                if self.resnet_dt_flag and layer.resnet_dt is not None:
                    x = torch.cat((x, x), dim=-1)  + hiden * layer.resnet_dt
                else:
                    x = torch.cat((x, x), dim=-1)  + hiden
            else:
                x = hiden

        return x
```

在这个例子中，我们创建了一个新的 `LayerModule` 类，它包含一个层的`weight`、`bias`和 `resnet_dt`（如果存在的话）。然后，`EmbeddingNet` 类创建了一个 `nn.ModuleList`，其中包含了所有的 `LayerModule` 实例。在 `forward` 方法中，我们可以方便地遍历 `self.layers`，并对每个 `LayerModule` 实例应用我们的逻辑。

这种方法的好处是，它允许 TorchScript 在编译时知道所有的索引和结构，因为我们不再依赖于动态的索引或键来访问我们的参数。
