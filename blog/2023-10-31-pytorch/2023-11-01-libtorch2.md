---
slug: libtorch2
title: libtorch修改使用记录2
authors: [hf]
tags: [python, libtorch]
---

使用 libtorch 时，对于一些函数的使用，以及数值类型的转换，需要注意一些问题。

## 1. 数据类型问题

如下所示代码：

```python
def get_train_2body_type(self, atom_type_data):
    type_2body_list = []
    type_2body_index = []
    for _ in atom_type_data:
        if _ != 0:
            type_2body_index.append(self.atom_type.index(_))

    for atom in type_2body_index:
        type_2body = []
        for atom2 in type_2body_index:
            type_2body.append([atom, atom2])
        type_2body_list.append(type_2body)
    return type_2body_list, len(type_2body_index)
```

```python
emb_list, type_nums =  self.get_train_2body_type(list(np.array(atom_type.cpu())[0]))
```

使用`torch.jit.script`将函数编译为 torchscript 时，会报错:

<!-- truncate -->

```python
RuntimeError:
Unknown type name 'List[numpy.int32]':
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 109
    def get_train_2body_type(self, atom_type_data):
                                   ~~~~~~~~~~~~~~ <--- HERE
        type_2body_list = []
        type_2body_index = []
'DP.get_train_2body_type' is being compiled since it was called from 'DP.forward'
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 147
        batch_size = Ri.shape[0]
        atom_sum = 0
        emb_list, type_nums =  self.get_train_2body_type(list(np.array(atom_type.cpu())[0]))
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
```

此错误表明 TorchScript 编译器在尝试编译代码时遇到了一个未知的类型 `'List[numpy.int32]'`。TorchScript 需要类型信息来编译和优化代码，但它不支持所有 Python 类型。尤其是，它不直接支持 `numpy` 类型，因为 `numpy` 是一个独立于 `PyTorch` 的库。因此将一个 `numpy` 数组直接传递给一个 TorchScript 函数或方法时，为了解决这个问题，需要将 `numpy` 数组转换为 TorchScript 支持的类型，如标准 `Python` 列表或 `PyTorch` 张量。

```python
emb_list, type_nums = self.get_train_2body_type(atom_type[0])
```

修改后，代码仍然报错：

```python
RuntimeError:

aten::append.t(t[](a!) self, t(c -> *) el) -> t[](a!):
Could not match type int to t in argument 'el': Type variable 't' previously matched to type Tensor is matched to type int.
:
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 114
        for _ in atom_type_data:
            if _ != 0:
                type_2body_index.append(self.atom_type.index(_))
                ~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE

        for atom in type_2body_index:
'DP.get_train_2body_type' is being compiled since it was called from 'DP.forward'
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 148
        atom_sum = 0
        # emb_list, type_nums =  self.get_train_2body_type(list(np.array(atom_type.cpu())[0]))
        emb_list, type_nums =  self.get_train_2body_type(atom_type.cpu().tolist()[0])
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE

```

问题出在 `type_2body_index.append(self.atom_type.index(_))` 这行代码上。TorchScript 试图将一个整数（从 `self.atom_type.index(_)` 返回）添加到一个它认为应该持有张量的列表中。

根据代码，`type_2body_index` 是一个整数列表，正在使用的 Python 的 `list.index()` 方法，该方法返回找到的元素的索引，这是一个`整数`。因此，TorchScript 抛出的错误表明在某个地方，`type_2body_index` 被推断为应该持有张量而不是整数。

要解决这个问题，我们需要确保 `type_2body_index` 在整个上下文中都被视为一个整数列表。此外，由于 TorchScript 的限制，我们需要避免在被 TorchScript 编译的函数中使用像 `list.index()` 这样的 Python 内建方法，因为 TorchScript 不支持所有的 Python 内建函数和方法。

下面是一种可能的修改方式，它避免了使用 `list.index()` 并清晰地指定了 `type_2body_index` 和 `type_2body_list` 应该持有的数据类型：

```python
def get_train_2body_type(self, atom_type_data: torch.Tensor) -> Tuple[List[List[int]], int]:
    type_2body_list: List[List[int]] = []
    type_2body_index: List[int] = []
    for atom_type in atom_type_data:
        if atom_type != 0:
            # 手动查找索引，而不是使用 list.index()
            for i, known_type in enumerate(self.atom_type):
                if atom_type == known_type:
                    type_2body_index.append(i)
                    break

    for atom in type_2body_index:
        type_2body = []
        for atom2 in type_2body_index:
            type_2body.append([atom, atom2])
        type_2body_list.append(type_2body)
    return type_2body_list, len(type_2body_index)

```

在这个修正中，添加了类型提示，明确指出 `atom_type_data` 是一个整数列表，而 `get_train_2body_type` 返回一个元组，该元组包含一个整数列表的列表和一个整数。

但是，这个修正仍然会导致 TorchScript 抛出一个错误：

```python
RuntimeError:

aten::append.t(t[](a!) self, t(c -> *) el) -> t[](a!):
Could not match type List[int] to t in argument 'el': Type variable 't' previously matched to type Tensor is matched to type List[int].
:
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 125
            type_2body = []
            for atom2 in type_2body_index:
                type_2body.append([atom, atom2])
                ~~~~~~~~~~~~~~~~~ <--- HERE
            type_2body_list.append(type_2body)
        return type_2body_list, len(type_2body_index)
```

此错误表明在尝试将一个类型为 `List[int]` 的项添加到预期为 `Tensor` 类型的列表时发生了类型不匹配。这是由于 TorchScript 期望列表中的所有项类型都相同，而在这种情况下，它检测到了一个不匹配的类型。

问题出在 `type_2body.append([atom, atom2])` 这行代码。看起来 `type_2body` 被推断为一个张量列表，但试图将一个整数列表`（[atom, atom2]）`添加到此列表中。

为了解决这个问题，需要保证 `type_2body` 和 `type_2body_list` 的类型在整个函数中保持一致。如果 `type_2body` 和 `type_2body_list` 都应该是整数列表的列表，需要确保这一点，并避免将它们与张量混淆。

下面是一种可能的修改方式，清晰地指定了 `type_2body` 和 `type_2body_list` 应该持有的数据类型：

```python
def get_train_2body_type(self, atom_type_data: torch.Tensor) -> Tuple[List[List[List[int]]], int]:
    type_2body_list: List[List[List[int]]] = []  # 修改这里，确保我们有一个整数列表的列表
    type_2body_index: List[int] = []
    for atom_type in atom_type_data:
        if atom_type != 0:
            for i, known_type in enumerate(self.atom_type):
                if atom_type == known_type:
                    type_2body_index.append(i)
                    break

    for atom in type_2body_index:
        type_2body: List[List[int]] = []  # 确保这是一个整数列表的列表
        for atom2 in type_2body_index:
            type_2body.append([atom, atom2])  # 这是一个整数列表
        type_2body_list.append(type_2body)  # 这里应该没有问题，因为 type_2body 是一个整数列表
    return type_2body_list, len(type_2body_index)

```

---

## 2. 循环、条件语句和变量作用域问题

代码继续往下执行，但是在下面的代码处又报错了：

这部分代码如下所示：

```python
Ei = None
for type_emb in emb_list:
    xyz_scater_a = None
    for emb in type_emb:
        ntype, ntype_1 = emb
        S_Rij = Ri[:, atom_sum:atom_sum+natoms[ntype], ntype_1 * self.maxNeighborNum:(ntype_1+1) * self.maxNeighborNum, 0].unsqueeze(-1)
        embedding_index = ntype * self.ntypes + ntype_1
        G = self.embedding_net[embedding_index](S_Rij)
        tmp_a = Ri[:, atom_sum:atom_sum+natoms[ntype], ntype_1 * self.maxNeighborNum:(ntype_1+1) * self.maxNeighborNum].transpose(-2, -1)
        tmp_b = torch.matmul(tmp_a, G)
        xyz_scater_a = tmp_b if xyz_scater_a is None else xyz_scater_a + tmp_b
```

```python
RuntimeError:
Expected integer literal for index but got a variable or non-integer. ModuleList/Sequential indexing is only supported with integer literals. For example, 'i = 4; self.layers[i](x)' will fail because i is not a literal. Enumeration is supported, e.g. 'for index, v in enumerate(self): out = v(inp)':
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 168
                G = self.embedding_net[embedding_index](S_Rij)
                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
```

这个错误是因为在 TorchScript 中，不能使用非字面量（non-literal）索引来索引 `ModuleList` 或 `Sequential`。代码中，`embedding_index` 和 `ntype` 是在运行时确定的变量，而 TorchScript 要求使用整数字面量（例如，直接使用 `self.layers[0]` 而不是 `self.layers[i]`，其中 `i` 是一个变量）

### 一种常见的解决方案是使用 enumerate 和一个循环，而不是直接索引：

```python
# ... 其他代码 ...

# self.embedding_net 是一个 ModuleList
for idx, emb_net in enumerate(self.embedding_net):
    if idx == embedding_index:
        G = emb_net(S_Rij)
        break

# ... 其他代码 ...

# self.fitting_net 是一个 ModuleList
for idx, fit_net in enumerate(self.fitting_net):
    if idx == ntype:
        Ei_ntype = fit_net(DR_ntype)
        break

# ... 其他代码 ...

```

使用 `enumerate` 函数和一个循环来遍历 `ModuleList` 的所有元素。当找到与 `embedding_index` 或 `ntype` 匹配的索引时，对应的网络被用于前向传递。这种方法可能会略微降低性能，因为它需要迭代整个列表，但这是一个 TorchScript 的限制。

但是，继续执行，发现代码又报错了：

```python
RuntimeError:
Because we emit iteration over modulelists or tuples as unrolled loops, we do not support break or continue inside the body of these loops:
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 170
                    if idx == embedding_index:
                        G = emb_net(S_Rij)
                        break
                        ~~~~~ <--- HERE
```

这是因为在 TorchScript 中，`ModuleList` 的循环迭代被视为完全展开的循环，因此不支持在这些循环中使用 `break`。

以下是一种可能的解决方案，它避免了使用 `break`：

```python
found = False
for idx, emb_net in enumerate(self.embedding_net):
    if idx == embedding_index and not found:
        G = emb_net(S_Rij)
        found = True

# ... 其他代码 ...

found = False
for idx, fit_net in enumerate(self.fitting_net):
    if idx == ntype and not found:
        Ei_ntype = fit_net(DR_ntype)
        found = True

```

注意，这里使用了一个布尔变量 `found`，它用于跟踪是否已经找到了匹配的索引。这种方法可能会略微降低性能，因为它需要迭代整个列表。

继续执行，发现代码又报错了：

```python
G is not defined in the false branch:
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 169
                found = False
                for idx, emb_net in enumerate(self.embedding_net):
                    if idx == embedding_index and not found:
                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
                        G = emb_net(S_Rij)
                        ~~~~~~~~~~~~~~~~~~
                        found = True
                        ~~~~~~~~~~~~ <--- HERE
```

这个错误发生是因为在 TorchScript 的静态图中，变量 `G` 必须在所有代码路径上都被定义。<font color='red'>由于 `if` 语句的条件可能不满足，TorchScript 认为存在一个代码分支在该分支中 `G` 没有被定义。要解决这个问题，你需要确保在所有分支上 `G` 都被定义。</font>

一种方法是在循环之前为 `G` 设置一个默认值:

```python
G = None
found = False
for idx, emb_net in enumerate(self.embedding_net):
    if idx == embedding_index and not found:
        G = emb_net(S_Rij)
        found = True
```

如上所示，直接为`G`初始化了一个`None`值，这样就可以保证在循环之前`G`已经被定义了。但是运行会发现，代码又报错了：

```python
RuntimeError:
Variable 'G' previously had type NoneType but is now being assigned to a value of type Tensor
:
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 387
                for idx, emb_net in enumerate(self.embedding_net):
                    if idx == embedding_index and not found:
                        G = emb_net(S_Rij)
                        ~ <--- HERE
                        found = True
            else:
'DP.calculate_xyz_scater' is being compiled since it was called from 'DP.calculate_Ei'
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 355
        atom_sum = 0
        for type_emb in emb_list:
            xyz_scater_a, xyz_scater_b, ntype = self.calculate_xyz_scater(Ri, atom_sum, natoms, type_emb, type_nums)
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
            DR_ntype = torch.matmul(xyz_scater_a.transpose(-2, -1), xyz_scater_b)
            DR_ntype = DR_ntype.reshape(batch_size, natoms[ntype], -1)
'DP.calculate_Ei' is being compiled since it was called from 'DP.forward'
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 329
        emb_list, type_nums = self.get_train_2body_type(atom_type[0])

        Ei = self.calculate_Ei(Ri, natoms, batch_size, emb_list, type_nums)
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
```

这个错误是因为 TorchScript 要求变量在其整个生命周期中具有一致的类型。当将 `G` 初始化为 `None` 时，它被推断为 `NoneType` 类型。随后，将一个 `Tensor` 分配给 `G` 时，这会与推断的类型冲突，从而导致错误。为了处理这个问题，应该使用 TorchScript 的类型注解来指定变量可能的类型。在这种情况下，`G` 可以是 `None` 或 `Tensor`。

下面是修改代码的方法:

```python
G: Optional[torch.Tensor] = None
found = False
for idx, emb_net in enumerate(self.embedding_net):
    if idx == embedding_index and not found:
        G = emb_net(S_Rij)
        found = True
```

这里使用了类型注解 `Optional[torch.Tensor]`，它表示 `G` 可以是 `None` 或 `Tensor`。这样，TorchScript 就可以推断出 `G` 的类型是 `Optional[torch.Tensor]`，并且在将 `G` 分配给 `emb_net(S_Rij)` 时，不会发生类型冲突。

但是需要注意的是，因为 `G` 可以是 `None`，所以在使用 `G` 时需要确保它不是 `None`。不然会报错：

```python
RuntimeError:
Arguments for call are not valid.
The following variants are available:

  aten::matmul(Tensor self, Tensor other) -> Tensor:
  Expected a value of type 'Tensor' for argument 'other' but instead found type 'Optional[Tensor]'.

  aten::matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!):
  Expected a value of type 'Tensor' for argument 'other' but instead found type 'Optional[Tensor]'.

The original call is:
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 400
            tmp_b = torch.matmul(tmp_a, G)
                    ~~~~~~~~~~~~ <--- HERE
            xyz_scater_a = tmp_b if xyz_scater_a is None else xyz_scater_a + tmp_b
'DP.calculate_xyz_scater' is being compiled since it was called from 'DP.calculate_Ei'
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 355
        atom_sum = 0
        for type_emb in emb_list:
            xyz_scater_a, xyz_scater_b, ntype = self.calculate_xyz_scater(Ri, atom_sum, natoms, type_emb, type_nums)
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
            DR_ntype = torch.matmul(xyz_scater_a.transpose(-2, -1), xyz_scater_b)
            DR_ntype = DR_ntype.reshape(batch_size, natoms[ntype], -1)
'DP.calculate_Ei' is being compiled since it was called from 'DP.forward'
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 329
        emb_list, type_nums = self.get_train_2body_type(atom_type[0])

        Ei = self.calculate_Ei(Ri, natoms, batch_size, emb_list, type_nums)
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE
```

报错代码出现在 `tmp_b = torch.matmul(tmp_a, G)` 这行代码上。这是因为 TorchScript 无法推断 `G` 不是 `None`，因此它将 `G` 的类型推断为 `Optional[torch.Tensor]`。因此，当将 `G` 传递给 `torch.matmul` 时，TorchScript 会报错，因为 `torch.matmul` 期望一个 `Tensor`，而不是 `Optional[torch.Tensor]`。

为了解决这个问题，必须确保`G`在传递给 `torch.matmul` 时不是 `None` 。而`G`通过初始化后在随后代码中经过了赋值，可以确保它不是 `None`。因此，可以使用 `assert` 语句来确保 `G` 不是 `None`。

```python
# ... 其他代码 ...
assert G is not None
tmp_b = torch.matmul(tmp_a, G)
```

或者，可以使用 `Optional` 类型的 `is not None` 检查：

```python
# ... 其他代码 ...
# if G is None:
#     raise ValueError("G has not been initialized properly.")
if G is not None:
    tmp_b = torch.matmul(tmp_a, G)
```

### 另一种解决方案是结合`dict`和 `enumerate` 来改变循环的结构，提前将网络存储在字典中，然后在循环中使用索引来访问网络

```python
...
emb_net_dict = {idx: emb_net for idx, emb_net in enumerate(self.embedding_net)}
for emb in type_emb:
    ntype, ntype_1 = emb
    ...
    embedding_index = ntype * self.ntypes + ntype_1
    emb_net = emb_net_dict.get(embedding_index)
    assert emb_net is not None
    G = emb_net.forward(S_Rij)
···
```

使用该方案获取网络索引时，需要确保索引在网络列表中存在(`assert emb_net is not None`)，否则会报错：

```python
RuntimeError:
'Optional[__torch__.src.model.dp_embedding.EmbeddingNet]' object has no attribute or method 'forward'.:
```

`emb_net` 是从 `emb_net_dict` 字典中获取的，其键是 `embedding_index`。如果 `embedding_index` 不在 `emb_net_dict` 的键中，`dict.get` 方法将返回 `None`，而`None` 没有 `forward` 方法，因此会报错。

:::note
通过这种方式调用模型的 `forward` 方法时，不能直接使用 `emb_net(S_Rij)`，而是需要使用 `emb_net.forward(S_Rij)`。

```python
RuntimeError:
'__torch__.src.model.dp_embedding.EmbeddingNet (of Python compilation unit at: 0x6ebce70)' object has no attribute or method '__call__'. Did you forget to initialize an attribute in __init__()?:
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 360
                emb_net = emb_net_dict.get(embedding_index)
                assert emb_net is not None
                G = emb_net(S_Rij)
                    ~~~~~~~ <--- HERE
```

在 PyTorch 中，通常我们通过调用模型实例（如 `model(input)`）来间接调用模型的 `forward` 方法，这实际上是调用模型的`call`方法，然后`call`方法再调用 `forward` 方法。在 TorchScript 中，模型的 `__call__` 方法（即当你调用 `emb_net(S_Rij)` 时实际调用的方法）可能无法正常工作，因为 TorchScript 可能无法正确解析和转换这个方法。这是因为 `__call__` 方法通常包含一些额外的逻辑（例如，调用 `forward` 方法之前的钩子函数），这些逻辑可能无法被 TorchScript 正确处理
:::

**为什么前面那种方法可以直接使用 `emb_net(S_Rij)`，且 TorchScript 可能无法正确解析和转换？ 这是因为在前面的方法中，之前的`emb_net` 是一个 `ModuleList`，而 `ModuleList` 的 `__call__` 方法是直接调用 `forward` 方法的，而不是调用 `__call__` 方法。因此，TorchScript 可以正确地将 `emb_net(S_Rij)` 转换为 `emb_net.forward(S_Rij)`。**

---

## 3.其他问题

继续执行，发现代码又报错了：

```python
RuntimeError:
undefined value ntype:
  File "/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py", line 189
            xyz_scater_b = xyz_scater_a[:, :, :, :self.M2]
            DR_ntype = torch.matmul(xyz_scater_a.transpose(-2, -1), xyz_scater_b)
            DR_ntype = DR_ntype.reshape(batch_size, natoms[ntype], -1)
                                                           ~~~~~ <--- HERE

            Ei_ntype = self.fitting_net[ntype](DR_ntype)

```

代码示例如下：

```python
Ei = None
for type_emb in emb_list:
    xyz_scater_a : Optional[torch.Tensor] = None
    for emb in type_emb:
        ntype, ntype_1 = emb
        S_Rij = Ri[:, atom_sum:atom_sum+natoms[ntype], ntype_1 * self.maxNeighborNum:(ntype_1+1) * self.maxNeighborNum, 0].unsqueeze(-1)
        embedding_index = ntype * self.ntypes + ntype_1
        G: Optional[torch.Tensor] = None
        found = False
        for idx, emb_net in enumerate(self.embedding_net):
            if idx == embedding_index and not found:
                G = emb_net(S_Rij)
                found = True

        tmp_a = Ri[:, atom_sum:atom_sum+natoms[ntype], ntype_1 * self.maxNeighborNum:(ntype_1+1) * self.maxNeighborNum].transpose(-2, -1)
        tmp_b = torch.matmul(tmp_a, G)
        xyz_scater_a = tmp_b if xyz_scater_a is None else xyz_scater_a + tmp_b

    xyz_scater_a = xyz_scater_a / (self.maxNeighborNum * type_nums)
    xyz_scater_b = xyz_scater_a[:, :, :, :self.M2]
    DR_ntype = torch.matmul(xyz_scater_a.transpose(-2, -1), xyz_scater_b)
    DR_ntype = DR_ntype.reshape(batch_size, natoms[ntype], -1)

    Ei_ntype: Optional[torch.Tensor] = None
    found = False
    for idx, fit_net in enumerate(self.fitting_net):
        if idx == ntype and not found:
            Ei_ntype = fit_net(DR_ntype)
            found = True
    Ei = Ei_ntype if Ei is None else torch.concat((Ei, Ei_ntype), dim=1)
    atom_sum = atom_sum + natoms[ntype]
```

错误信息表明在使用变量 `ntype` 时，它的值是未定义的。这可能是由于在你的代码的某个部分，特别是在循环或条件语句之外使用了 `ntype`，而在这些情况下，`ntype` 的值可能尚未被分配。

错误发生在以下行：

```python
DR_ntype = DR_ntype.reshape(batch_size, natoms[ntype], -1)
```

此处 `ntype` 应该是定义好的，因为它是在前面的循环中定义的：

```python
for emb in type_emb:
    ntype, ntype_1 = emb
    ...

```

由于 TorchScript 需要所有变量在它们被使用之前都有明确的定义，这种错误通常发生在变量可能未被初始化的情况,是 TorchScript 编译器无法正确推断 `ntype` 的值。一个可能的解决方案是确保 `ntype` 在循环外有一个默认值。

```python
ntype = 0  # 或任何适当的默认值
for emb in type_emb:
    ntype, ntype_1 = emb
    ...
```

如果 `type_emb` 有可能为空，这将确保 `ntype` 总是有一个定义好的值。请注意，这只是一个通用解决方案，需要根据你的具体情况来决定最合适的默认值。
