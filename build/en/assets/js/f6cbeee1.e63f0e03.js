"use strict";(self.webpackChunkhf_log=self.webpackChunkhf_log||[]).push([[3836],{4641:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"libtorch2","metadata":{"permalink":"/en/blog/libtorch2","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-10-31-pytorch/2023-11-01-libtorch2.md","source":"@site/blog/2023-10-31-pytorch/2023-11-01-libtorch2.md","title":"libtorch\u4fee\u6539\u4f7f\u7528\u8bb0\u5f552","description":"\u4f7f\u7528 libtorch \u65f6\uff0c\u5bf9\u4e8e\u4e00\u4e9b\u51fd\u6570\u7684\u4f7f\u7528\uff0c\u4ee5\u53ca\u6570\u503c\u7c7b\u578b\u7684\u8f6c\u6362\uff0c\u9700\u8981\u6ce8\u610f\u4e00\u4e9b\u95ee\u9898\u3002","date":"2023-11-01T00:00:00.000Z","formattedDate":"November 1, 2023","tags":[{"label":"python","permalink":"/en/blog/tags/python"},{"label":"libtorch","permalink":"/en/blog/tags/libtorch"}],"readingTime":15.78,"hasTruncateMarker":false,"authors":[{"name":"\u55b5s\u01ceng~","title":"\u4e00\u4ecb\u8349\u6c11","url":"https://github.com/yeahooool","imageURL":"https://z1.ax1x.com/2023/10/31/pinJO8H.jpg","key":"hf"}],"frontMatter":{"slug":"libtorch2","title":"libtorch\u4fee\u6539\u4f7f\u7528\u8bb0\u5f552","authors":["hf"],"tags":["python","libtorch"]},"nextItem":{"title":"libtorch\u4fee\u6539\u4f7f\u7528\u8bb0\u5f55","permalink":"/en/blog/libtorch1"}},"content":"\u4f7f\u7528 libtorch \u65f6\uff0c\u5bf9\u4e8e\u4e00\u4e9b\u51fd\u6570\u7684\u4f7f\u7528\uff0c\u4ee5\u53ca\u6570\u503c\u7c7b\u578b\u7684\u8f6c\u6362\uff0c\u9700\u8981\u6ce8\u610f\u4e00\u4e9b\u95ee\u9898\u3002\\r\\n\\r\\n\u5982\u4e0b\u6240\u793a\u4ee3\u7801\uff1a\\r\\n\\r\\n```python\\r\\ndef get_train_2body_type(self, atom_type_data):\\r\\n    type_2body_list = []\\r\\n    type_2body_index = []\\r\\n    for _ in atom_type_data:\\r\\n        if _ != 0:\\r\\n            type_2body_index.append(self.atom_type.index(_))\\r\\n\\r\\n    for atom in type_2body_index:\\r\\n        type_2body = []\\r\\n        for atom2 in type_2body_index:\\r\\n            type_2body.append([atom, atom2])\\r\\n        type_2body_list.append(type_2body)\\r\\n    return type_2body_list, len(type_2body_index)\\r\\n```\\r\\n\\r\\n```python\\r\\nemb_list, type_nums =  self.get_train_2body_type(list(np.array(atom_type.cpu())[0]))\\r\\n```\\r\\n\\r\\n\u4f7f\u7528`torch.jit.script`\u5c06\u51fd\u6570\u7f16\u8bd1\u4e3a torchscript \u65f6\uff0c\u4f1a\u62a5\u9519:\\r\\n\\r\\n```python\\r\\nRuntimeError:\\r\\nUnknown type name \'List[numpy.int32]\':\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 109\\r\\n    def get_train_2body_type(self, atom_type_data):\\r\\n                                   ~~~~~~~~~~~~~~ <--- HERE\\r\\n        type_2body_list = []\\r\\n        type_2body_index = []\\r\\n\'DP.get_train_2body_type\' is being compiled since it was called from \'DP.forward\'\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 147\\r\\n        batch_size = Ri.shape[0]\\r\\n        atom_sum = 0\\r\\n        emb_list, type_nums =  self.get_train_2body_type(list(np.array(atom_type.cpu())[0]))\\r\\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\\r\\n```\\r\\n\\r\\n\u6b64\u9519\u8bef\u8868\u660e TorchScript \u7f16\u8bd1\u5668\u5728\u5c1d\u8bd5\u7f16\u8bd1\u4ee3\u7801\u65f6\u9047\u5230\u4e86\u4e00\u4e2a\u672a\u77e5\u7684\u7c7b\u578b `\'List[numpy.int32]\'`\u3002TorchScript \u9700\u8981\u7c7b\u578b\u4fe1\u606f\u6765\u7f16\u8bd1\u548c\u4f18\u5316\u4ee3\u7801\uff0c\u4f46\u5b83\u4e0d\u652f\u6301\u6240\u6709 Python \u7c7b\u578b\u3002\u5c24\u5176\u662f\uff0c\u5b83\u4e0d\u76f4\u63a5\u652f\u6301 `numpy` \u7c7b\u578b\uff0c\u56e0\u4e3a `numpy` \u662f\u4e00\u4e2a\u72ec\u7acb\u4e8e `PyTorch` \u7684\u5e93\u3002\u56e0\u6b64\u5c06\u4e00\u4e2a `numpy` \u6570\u7ec4\u76f4\u63a5\u4f20\u9012\u7ed9\u4e00\u4e2a TorchScript \u51fd\u6570\u6216\u65b9\u6cd5\u65f6\uff0c\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u5c06 `numpy` \u6570\u7ec4\u8f6c\u6362\u4e3a TorchScript \u652f\u6301\u7684\u7c7b\u578b\uff0c\u5982\u6807\u51c6 `Python` \u5217\u8868\u6216 `PyTorch` \u5f20\u91cf\u3002\\r\\n\\r\\n```python\\r\\nemb_list, type_nums = self.get_train_2body_type(atom_type[0])\\r\\n```\\r\\n\\r\\n\u4fee\u6539\u540e\uff0c\u4ee3\u7801\u4ecd\u7136\u62a5\u9519\uff1a\\r\\n\\r\\n```python\\r\\nRuntimeError:\\r\\n\\r\\naten::append.t(t[](a!) self, t(c -> *) el) -> t[](a!):\\r\\nCould not match type int to t in argument \'el\': Type variable \'t\' previously matched to type Tensor is matched to type int.\\r\\n:\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 114\\r\\n        for _ in atom_type_data:\\r\\n            if _ != 0:\\r\\n                type_2body_index.append(self.atom_type.index(_))\\r\\n                ~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\\r\\n\\r\\n        for atom in type_2body_index:\\r\\n\'DP.get_train_2body_type\' is being compiled since it was called from \'DP.forward\'\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 148\\r\\n        atom_sum = 0\\r\\n        # emb_list, type_nums =  self.get_train_2body_type(list(np.array(atom_type.cpu())[0]))\\r\\n        emb_list, type_nums =  self.get_train_2body_type(atom_type.cpu().tolist()[0])\\r\\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\\r\\n\\r\\n```\\r\\n\\r\\n\u95ee\u9898\u51fa\u5728 `type_2body_index.append(self.atom_type.index(_))` \u8fd9\u884c\u4ee3\u7801\u4e0a\u3002TorchScript \u8bd5\u56fe\u5c06\u4e00\u4e2a\u6574\u6570\uff08\u4ece `self.atom_type.index(_)` \u8fd4\u56de\uff09\u6dfb\u52a0\u5230\u4e00\u4e2a\u5b83\u8ba4\u4e3a\u5e94\u8be5\u6301\u6709\u5f20\u91cf\u7684\u5217\u8868\u4e2d\u3002\\r\\n\\r\\n\u6839\u636e\u4ee3\u7801\uff0c`type_2body_index` \u662f\u4e00\u4e2a\u6574\u6570\u5217\u8868\uff0c\u6b63\u5728\u4f7f\u7528\u7684 Python \u7684 `list.index()` \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u8fd4\u56de\u627e\u5230\u7684\u5143\u7d20\u7684\u7d22\u5f15\uff0c\u8fd9\u662f\u4e00\u4e2a`\u6574\u6570`\u3002\u56e0\u6b64\uff0cTorchScript \u629b\u51fa\u7684\u9519\u8bef\u8868\u660e\u5728\u67d0\u4e2a\u5730\u65b9\uff0c`type_2body_index` \u88ab\u63a8\u65ad\u4e3a\u5e94\u8be5\u6301\u6709\u5f20\u91cf\u800c\u4e0d\u662f\u6574\u6570\u3002\\r\\n\\r\\n\u8981\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u9700\u8981\u786e\u4fdd `type_2body_index` \u5728\u6574\u4e2a\u4e0a\u4e0b\u6587\u4e2d\u90fd\u88ab\u89c6\u4e3a\u4e00\u4e2a\u6574\u6570\u5217\u8868\u3002\u6b64\u5916\uff0c\u7531\u4e8e TorchScript \u7684\u9650\u5236\uff0c\u6211\u4eec\u9700\u8981\u907f\u514d\u5728\u88ab TorchScript \u7f16\u8bd1\u7684\u51fd\u6570\u4e2d\u4f7f\u7528\u50cf `list.index()` \u8fd9\u6837\u7684 Python \u5185\u5efa\u65b9\u6cd5\uff0c\u56e0\u4e3a TorchScript \u4e0d\u652f\u6301\u6240\u6709\u7684 Python \u5185\u5efa\u51fd\u6570\u548c\u65b9\u6cd5\u3002\\r\\n\\r\\n\u4e0b\u9762\u662f\u4e00\u79cd\u53ef\u80fd\u7684\u4fee\u6539\u65b9\u5f0f\uff0c\u5b83\u907f\u514d\u4e86\u4f7f\u7528 `list.index()` \u5e76\u6e05\u6670\u5730\u6307\u5b9a\u4e86 `type_2body_index` \u548c `type_2body_list` \u5e94\u8be5\u6301\u6709\u7684\u6570\u636e\u7c7b\u578b\uff1a\\r\\n\\r\\n```python\\r\\ndef get_train_2body_type(self, atom_type_data: torch.Tensor) -> Tuple[List[List[int]], int]:\\r\\n    type_2body_list: List[List[int]] = []\\r\\n    type_2body_index: List[int] = []\\r\\n    for atom_type in atom_type_data:\\r\\n        if atom_type != 0:\\r\\n            # \u624b\u52a8\u67e5\u627e\u7d22\u5f15\uff0c\u800c\u4e0d\u662f\u4f7f\u7528 list.index()\\r\\n            for i, known_type in enumerate(self.atom_type):\\r\\n                if atom_type == known_type:\\r\\n                    type_2body_index.append(i)\\r\\n                    break\\r\\n\\r\\n    for atom in type_2body_index:\\r\\n        type_2body = []\\r\\n        for atom2 in type_2body_index:\\r\\n            type_2body.append([atom, atom2])\\r\\n        type_2body_list.append(type_2body)\\r\\n    return type_2body_list, len(type_2body_index)\\r\\n\\r\\n```\\r\\n\\r\\n\u5728\u8fd9\u4e2a\u4fee\u6b63\u4e2d\uff0c\u6dfb\u52a0\u4e86\u7c7b\u578b\u63d0\u793a\uff0c\u660e\u786e\u6307\u51fa `atom_type_data` \u662f\u4e00\u4e2a\u6574\u6570\u5217\u8868\uff0c\u800c `get_train_2body_type` \u8fd4\u56de\u4e00\u4e2a\u5143\u7ec4\uff0c\u8be5\u5143\u7ec4\u5305\u542b\u4e00\u4e2a\u6574\u6570\u5217\u8868\u7684\u5217\u8868\u548c\u4e00\u4e2a\u6574\u6570\u3002\\r\\n\\r\\n\u4f46\u662f\uff0c\u8fd9\u4e2a\u4fee\u6b63\u4ecd\u7136\u4f1a\u5bfc\u81f4 TorchScript \u629b\u51fa\u4e00\u4e2a\u9519\u8bef\uff1a\\r\\n\\r\\n```python\\r\\nRuntimeError:\\r\\n\\r\\naten::append.t(t[](a!) self, t(c -> *) el) -> t[](a!):\\r\\nCould not match type List[int] to t in argument \'el\': Type variable \'t\' previously matched to type Tensor is matched to type List[int].\\r\\n:\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 125\\r\\n            type_2body = []\\r\\n            for atom2 in type_2body_index:\\r\\n                type_2body.append([atom, atom2])\\r\\n                ~~~~~~~~~~~~~~~~~ <--- HERE\\r\\n            type_2body_list.append(type_2body)\\r\\n        return type_2body_list, len(type_2body_index)\\r\\n```\\r\\n\\r\\n\u6b64\u9519\u8bef\u8868\u660e\u5728\u5c1d\u8bd5\u5c06\u4e00\u4e2a\u7c7b\u578b\u4e3a `List[int]` \u7684\u9879\u6dfb\u52a0\u5230\u9884\u671f\u4e3a `Tensor` \u7c7b\u578b\u7684\u5217\u8868\u65f6\u53d1\u751f\u4e86\u7c7b\u578b\u4e0d\u5339\u914d\u3002\u8fd9\u662f\u7531\u4e8e TorchScript \u671f\u671b\u5217\u8868\u4e2d\u7684\u6240\u6709\u9879\u7c7b\u578b\u90fd\u76f8\u540c\uff0c\u800c\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5b83\u68c0\u6d4b\u5230\u4e86\u4e00\u4e2a\u4e0d\u5339\u914d\u7684\u7c7b\u578b\u3002\\r\\n\\r\\n\u95ee\u9898\u51fa\u5728 `type_2body.append([atom, atom2])` \u8fd9\u884c\u4ee3\u7801\u3002\u770b\u8d77\u6765 `type_2body` \u88ab\u63a8\u65ad\u4e3a\u4e00\u4e2a\u5f20\u91cf\u5217\u8868\uff0c\u4f46\u8bd5\u56fe\u5c06\u4e00\u4e2a\u6574\u6570\u5217\u8868`\uff08[atom, atom2]\uff09`\u6dfb\u52a0\u5230\u6b64\u5217\u8868\u4e2d\u3002\\r\\n\\r\\n\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u9700\u8981\u4fdd\u8bc1 `type_2body` \u548c `type_2body_list` \u7684\u7c7b\u578b\u5728\u6574\u4e2a\u51fd\u6570\u4e2d\u4fdd\u6301\u4e00\u81f4\u3002\u5982\u679c `type_2body` \u548c `type_2body_list` \u90fd\u5e94\u8be5\u662f\u6574\u6570\u5217\u8868\u7684\u5217\u8868\uff0c\u9700\u8981\u786e\u4fdd\u8fd9\u4e00\u70b9\uff0c\u5e76\u907f\u514d\u5c06\u5b83\u4eec\u4e0e\u5f20\u91cf\u6df7\u6dc6\u3002\\r\\n\\r\\n\u4e0b\u9762\u662f\u4e00\u79cd\u53ef\u80fd\u7684\u4fee\u6539\u65b9\u5f0f\uff0c\u6e05\u6670\u5730\u6307\u5b9a\u4e86 `type_2body` \u548c `type_2body_list` \u5e94\u8be5\u6301\u6709\u7684\u6570\u636e\u7c7b\u578b\uff1a\\r\\n\\r\\n```python\\r\\ndef get_train_2body_type(self, atom_type_data: torch.Tensor) -> Tuple[List[List[List[int]]], int]:\\r\\n    type_2body_list: List[List[List[int]]] = []  # \u4fee\u6539\u8fd9\u91cc\uff0c\u786e\u4fdd\u6211\u4eec\u6709\u4e00\u4e2a\u6574\u6570\u5217\u8868\u7684\u5217\u8868\\r\\n    type_2body_index: List[int] = []\\r\\n    for atom_type in atom_type_data:\\r\\n        if atom_type != 0:\\r\\n            for i, known_type in enumerate(self.atom_type):\\r\\n                if atom_type == known_type:\\r\\n                    type_2body_index.append(i)\\r\\n                    break\\r\\n\\r\\n    for atom in type_2body_index:\\r\\n        type_2body: List[List[int]] = []  # \u786e\u4fdd\u8fd9\u662f\u4e00\u4e2a\u6574\u6570\u5217\u8868\u7684\u5217\u8868\\r\\n        for atom2 in type_2body_index:\\r\\n            type_2body.append([atom, atom2])  # \u8fd9\u662f\u4e00\u4e2a\u6574\u6570\u5217\u8868\\r\\n        type_2body_list.append(type_2body)  # \u8fd9\u91cc\u5e94\u8be5\u6ca1\u6709\u95ee\u9898\uff0c\u56e0\u4e3a type_2body \u662f\u4e00\u4e2a\u6574\u6570\u5217\u8868\\r\\n    return type_2body_list, len(type_2body_index)\\r\\n\\r\\n```\\r\\n\\r\\n---\\r\\n\\r\\n\u4ee3\u7801\u7ee7\u7eed\u5f80\u4e0b\u6267\u884c\uff0c\u4f46\u662f\u5728\u4e0b\u9762\u7684\u4ee3\u7801\u5904\u53c8\u62a5\u9519\u4e86\uff1a\\r\\n\\r\\n\u8fd9\u90e8\u5206\u4ee3\u7801\u5982\u4e0b\u6240\u793a\uff1a\\r\\n\\r\\n```python\\r\\nEi = None\\r\\nfor type_emb in emb_list:\\r\\n    xyz_scater_a = None\\r\\n    for emb in type_emb:\\r\\n        ntype, ntype_1 = emb\\r\\n        S_Rij = Ri[:, atom_sum:atom_sum+natoms[ntype], ntype_1 * self.maxNeighborNum:(ntype_1+1) * self.maxNeighborNum, 0].unsqueeze(-1)\\r\\n        embedding_index = ntype * self.ntypes + ntype_1\\r\\n        G = self.embedding_net[embedding_index](S_Rij)\\r\\n        tmp_a = Ri[:, atom_sum:atom_sum+natoms[ntype], ntype_1 * self.maxNeighborNum:(ntype_1+1) * self.maxNeighborNum].transpose(-2, -1)\\r\\n        tmp_b = torch.matmul(tmp_a, G)\\r\\n        xyz_scater_a = tmp_b if xyz_scater_a is None else xyz_scater_a + tmp_b\\r\\n```\\r\\n\\r\\n```python\\r\\nRuntimeError:\\r\\nExpected integer literal for index but got a variable or non-integer. ModuleList/Sequential indexing is only supported with integer literals. For example, \'i = 4; self.layers[i](x)\' will fail because i is not a literal. Enumeration is supported, e.g. \'for index, v in enumerate(self): out = v(inp)\':\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 168\\r\\n                G = self.embedding_net[embedding_index](S_Rij)\\r\\n                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\\r\\n```\\r\\n\\r\\n\u8fd9\u4e2a\u9519\u8bef\u662f\u56e0\u4e3a\u5728 TorchScript \u4e2d\uff0c\u4e0d\u80fd\u4f7f\u7528\u975e\u5b57\u9762\u91cf\uff08non-literal\uff09\u7d22\u5f15\u6765\u7d22\u5f15 `ModuleList` \u6216 `Sequential`\u3002\u4ee3\u7801\u4e2d\uff0c`embedding_index` \u548c `ntype` \u662f\u5728\u8fd0\u884c\u65f6\u786e\u5b9a\u7684\u53d8\u91cf\uff0c\u800c TorchScript \u8981\u6c42\u4f7f\u7528\u6574\u6570\u5b57\u9762\u91cf\uff08\u4f8b\u5982\uff0c\u76f4\u63a5\u4f7f\u7528 `self.layers[0]` \u800c\u4e0d\u662f `self.layers[i]`\uff0c\u5176\u4e2d `i` \u662f\u4e00\u4e2a\u53d8\u91cf\uff09\\r\\n\\r\\n\u4e00\u79cd\u5e38\u89c1\u7684\u89e3\u51b3\u65b9\u6848\u662f\u4f7f\u7528 enumerate \u548c\u4e00\u4e2a\u5faa\u73af\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u7d22\u5f15\uff1a\\r\\n\\r\\n```python\\r\\n# ... \u5176\u4ed6\u4ee3\u7801 ...\\r\\n\\r\\n# self.embedding_net \u662f\u4e00\u4e2a ModuleList\\r\\nfor idx, emb_net in enumerate(self.embedding_net):\\r\\n    if idx == embedding_index:\\r\\n        G = emb_net(S_Rij)\\r\\n        break\\r\\n\\r\\n# ... \u5176\u4ed6\u4ee3\u7801 ...\\r\\n\\r\\n# self.fitting_net \u662f\u4e00\u4e2a ModuleList\\r\\nfor idx, fit_net in enumerate(self.fitting_net):\\r\\n    if idx == ntype:\\r\\n        Ei_ntype = fit_net(DR_ntype)\\r\\n        break\\r\\n\\r\\n# ... \u5176\u4ed6\u4ee3\u7801 ...\\r\\n\\r\\n```\\r\\n\\r\\n\u4f7f\u7528 `enumerate` \u51fd\u6570\u548c\u4e00\u4e2a\u5faa\u73af\u6765\u904d\u5386 `ModuleList` \u7684\u6240\u6709\u5143\u7d20\u3002\u5f53\u627e\u5230\u4e0e `embedding_index` \u6216 `ntype` \u5339\u914d\u7684\u7d22\u5f15\u65f6\uff0c\u5bf9\u5e94\u7684\u7f51\u7edc\u88ab\u7528\u4e8e\u524d\u5411\u4f20\u9012\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u4f1a\u7565\u5fae\u964d\u4f4e\u6027\u80fd\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u8fed\u4ee3\u6574\u4e2a\u5217\u8868\uff0c\u4f46\u8fd9\u662f\u4e00\u4e2a TorchScript \u7684\u9650\u5236\u3002\\r\\n\\r\\n\u4f46\u662f\uff0c\u7ee7\u7eed\u6267\u884c\uff0c\u53d1\u73b0\u4ee3\u7801\u53c8\u62a5\u9519\u4e86\uff1a\\r\\n\\r\\n```python\\r\\nRuntimeError:\\r\\nBecause we emit iteration over modulelists or tuples as unrolled loops, we do not support break or continue inside the body of these loops:\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 170\\r\\n                    if idx == embedding_index:\\r\\n                        G = emb_net(S_Rij)\\r\\n                        break\\r\\n                        ~~~~~ <--- HERE\\r\\n```\\r\\n\\r\\n\u8fd9\u662f\u56e0\u4e3a\u5728 TorchScript \u4e2d\uff0c`ModuleList` \u7684\u5faa\u73af\u8fed\u4ee3\u88ab\u89c6\u4e3a\u5b8c\u5168\u5c55\u5f00\u7684\u5faa\u73af\uff0c\u56e0\u6b64\u4e0d\u652f\u6301\u5728\u8fd9\u4e9b\u5faa\u73af\u4e2d\u4f7f\u7528 `break`\u3002\\r\\n\\r\\n\u4ee5\u4e0b\u662f\u4e00\u79cd\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u5b83\u907f\u514d\u4e86\u4f7f\u7528 `break`\uff1a\\r\\n\\r\\n```python\\r\\nfound = False\\r\\nfor idx, emb_net in enumerate(self.embedding_net):\\r\\n    if idx == embedding_index and not found:\\r\\n        G = emb_net(S_Rij)\\r\\n        found = True\\r\\n\\r\\n# ... \u5176\u4ed6\u4ee3\u7801 ...\\r\\n\\r\\nfound = False\\r\\nfor idx, fit_net in enumerate(self.fitting_net):\\r\\n    if idx == ntype and not found:\\r\\n        Ei_ntype = fit_net(DR_ntype)\\r\\n        found = True\\r\\n\\r\\n```\\r\\n\\r\\n\u6ce8\u610f\uff0c\u8fd9\u91cc\u4f7f\u7528\u4e86\u4e00\u4e2a\u5e03\u5c14\u53d8\u91cf `found`\uff0c\u5b83\u7528\u4e8e\u8ddf\u8e2a\u662f\u5426\u5df2\u7ecf\u627e\u5230\u4e86\u5339\u914d\u7684\u7d22\u5f15\u3002\u8fd9\u79cd\u65b9\u6cd5\u53ef\u80fd\u4f1a\u7565\u5fae\u964d\u4f4e\u6027\u80fd\uff0c\u56e0\u4e3a\u5b83\u9700\u8981\u8fed\u4ee3\u6574\u4e2a\u5217\u8868\u3002\\r\\n\\r\\n\u7ee7\u7eed\u6267\u884c\uff0c\u53d1\u73b0\u4ee3\u7801\u53c8\u62a5\u9519\u4e86\uff1a\\r\\n\\r\\n```python\\r\\nG is not defined in the false branch:\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 169\\r\\n                found = False\\r\\n                for idx, emb_net in enumerate(self.embedding_net):\\r\\n                    if idx == embedding_index and not found:\\r\\n                    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\\r\\n                        G = emb_net(S_Rij)\\r\\n                        ~~~~~~~~~~~~~~~~~~\\r\\n                        found = True\\r\\n                        ~~~~~~~~~~~~ <--- HERE\\r\\n```\\r\\n\\r\\n\u8fd9\u4e2a\u9519\u8bef\u53d1\u751f\u662f\u56e0\u4e3a\u5728 TorchScript \u7684\u9759\u6001\u56fe\u4e2d\uff0c\u53d8\u91cf `G` \u5fc5\u987b\u5728\u6240\u6709\u4ee3\u7801\u8def\u5f84\u4e0a\u90fd\u88ab\u5b9a\u4e49\u3002<font color=\'red\'>\u7531\u4e8e `if` \u8bed\u53e5\u7684\u6761\u4ef6\u53ef\u80fd\u4e0d\u6ee1\u8db3\uff0cTorchScript \u8ba4\u4e3a\u5b58\u5728\u4e00\u4e2a\u4ee3\u7801\u5206\u652f\u5728\u8be5\u5206\u652f\u4e2d `G` \u6ca1\u6709\u88ab\u5b9a\u4e49\u3002\u8981\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u4f60\u9700\u8981\u786e\u4fdd\u5728\u6240\u6709\u5206\u652f\u4e0a `G` \u90fd\u88ab\u5b9a\u4e49\u3002</font>\\r\\n\\r\\n\u4e00\u79cd\u65b9\u6cd5\u662f\u5728\u5faa\u73af\u4e4b\u524d\u4e3a `G` \u8bbe\u7f6e\u4e00\u4e2a\u9ed8\u8ba4\u503c:\\r\\n\\r\\n```python\\r\\nG = None\\r\\nfound = False\\r\\nfor idx, emb_net in enumerate(self.embedding_net):\\r\\n    if idx == embedding_index and not found:\\r\\n        G = emb_net(S_Rij)\\r\\n        found = True\\r\\n```\\r\\n\\r\\n\u5982\u4e0a\u6240\u793a\uff0c\u76f4\u63a5\u4e3a`G`\u521d\u59cb\u5316\u4e86\u4e00\u4e2a`None`\u503c\uff0c\u8fd9\u6837\u5c31\u53ef\u4ee5\u4fdd\u8bc1\u5728\u5faa\u73af\u4e4b\u524d`G`\u5df2\u7ecf\u88ab\u5b9a\u4e49\u4e86\u3002\u4f46\u662f\u8fd0\u884c\u4f1a\u53d1\u73b0\uff0c\u4ee3\u7801\u53c8\u62a5\u9519\u4e86\uff1a\\r\\n\\r\\n```python\\r\\nRuntimeError:\\r\\nVariable \'G\' previously had type NoneType but is now being assigned to a value of type Tensor\\r\\n:\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 387\\r\\n                for idx, emb_net in enumerate(self.embedding_net):\\r\\n                    if idx == embedding_index and not found:\\r\\n                        G = emb_net(S_Rij)\\r\\n                        ~ <--- HERE\\r\\n                        found = True\\r\\n            else:\\r\\n\'DP.calculate_xyz_scater\' is being compiled since it was called from \'DP.calculate_Ei\'\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 355\\r\\n        atom_sum = 0\\r\\n        for type_emb in emb_list:\\r\\n            xyz_scater_a, xyz_scater_b, ntype = self.calculate_xyz_scater(Ri, atom_sum, natoms, type_emb, type_nums)\\r\\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\\r\\n            DR_ntype = torch.matmul(xyz_scater_a.transpose(-2, -1), xyz_scater_b)\\r\\n            DR_ntype = DR_ntype.reshape(batch_size, natoms[ntype], -1)\\r\\n\'DP.calculate_Ei\' is being compiled since it was called from \'DP.forward\'\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 329\\r\\n        emb_list, type_nums = self.get_train_2body_type(atom_type[0])\\r\\n\\r\\n        Ei = self.calculate_Ei(Ri, natoms, batch_size, emb_list, type_nums)\\r\\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\\r\\n```\\r\\n\\r\\n\u8fd9\u4e2a\u9519\u8bef\u662f\u56e0\u4e3a TorchScript \u8981\u6c42\u53d8\u91cf\u5728\u5176\u6574\u4e2a\u751f\u547d\u5468\u671f\u4e2d\u5177\u6709\u4e00\u81f4\u7684\u7c7b\u578b\u3002\u5f53\u5c06 `G` \u521d\u59cb\u5316\u4e3a `None` \u65f6\uff0c\u5b83\u88ab\u63a8\u65ad\u4e3a `NoneType` \u7c7b\u578b\u3002\u968f\u540e\uff0c\u5c06\u4e00\u4e2a `Tensor` \u5206\u914d\u7ed9 `G` \u65f6\uff0c\u8fd9\u4f1a\u4e0e\u63a8\u65ad\u7684\u7c7b\u578b\u51b2\u7a81\uff0c\u4ece\u800c\u5bfc\u81f4\u9519\u8bef\u3002\u4e3a\u4e86\u5904\u7406\u8fd9\u4e2a\u95ee\u9898\uff0c\u5e94\u8be5\u4f7f\u7528 TorchScript \u7684\u7c7b\u578b\u6ce8\u89e3\u6765\u6307\u5b9a\u53d8\u91cf\u53ef\u80fd\u7684\u7c7b\u578b\u3002\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c`G` \u53ef\u4ee5\u662f `None` \u6216 `Tensor`\u3002\\r\\n\\r\\n\u4e0b\u9762\u662f\u4fee\u6539\u4ee3\u7801\u7684\u65b9\u6cd5:\\r\\n\\r\\n```python\\r\\nG: Optional[torch.Tensor] = None\\r\\nfound = False\\r\\nfor idx, emb_net in enumerate(self.embedding_net):\\r\\n    if idx == embedding_index and not found:\\r\\n        G = emb_net(S_Rij)\\r\\n        found = True\\r\\n```\\r\\n\\r\\n\u8fd9\u91cc\u4f7f\u7528\u4e86\u7c7b\u578b\u6ce8\u89e3 `Optional[torch.Tensor]`\uff0c\u5b83\u8868\u793a `G` \u53ef\u4ee5\u662f `None` \u6216 `Tensor`\u3002\u8fd9\u6837\uff0cTorchScript \u5c31\u53ef\u4ee5\u63a8\u65ad\u51fa `G` \u7684\u7c7b\u578b\u662f `Optional[torch.Tensor]`\uff0c\u5e76\u4e14\u5728\u5c06 `G` \u5206\u914d\u7ed9 `emb_net(S_Rij)` \u65f6\uff0c\u4e0d\u4f1a\u53d1\u751f\u7c7b\u578b\u51b2\u7a81\u3002\\r\\n\\r\\n\u4f46\u662f\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u56e0\u4e3a `G` \u53ef\u4ee5\u662f `None`\uff0c\u6240\u4ee5\u5728\u4f7f\u7528 `G` \u65f6\u9700\u8981\u786e\u4fdd\u5b83\u4e0d\u662f `None`\u3002\u4e0d\u7136\u4f1a\u62a5\u9519\uff1a\\r\\n\\r\\n```python\\r\\nRuntimeError:\\r\\nArguments for call are not valid.\\r\\nThe following variants are available:\\r\\n\\r\\n  aten::matmul(Tensor self, Tensor other) -> Tensor:\\r\\n  Expected a value of type \'Tensor\' for argument \'other\' but instead found type \'Optional[Tensor]\'.\\r\\n\\r\\n  aten::matmul.out(Tensor self, Tensor other, *, Tensor(a!) out) -> Tensor(a!):\\r\\n  Expected a value of type \'Tensor\' for argument \'other\' but instead found type \'Optional[Tensor]\'.\\r\\n\\r\\nThe original call is:\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 400\\r\\n            tmp_b = torch.matmul(tmp_a, G)\\r\\n                    ~~~~~~~~~~~~ <--- HERE\\r\\n            xyz_scater_a = tmp_b if xyz_scater_a is None else xyz_scater_a + tmp_b\\r\\n\'DP.calculate_xyz_scater\' is being compiled since it was called from \'DP.calculate_Ei\'\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 355\\r\\n        atom_sum = 0\\r\\n        for type_emb in emb_list:\\r\\n            xyz_scater_a, xyz_scater_b, ntype = self.calculate_xyz_scater(Ri, atom_sum, natoms, type_emb, type_nums)\\r\\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\\r\\n            DR_ntype = torch.matmul(xyz_scater_a.transpose(-2, -1), xyz_scater_b)\\r\\n            DR_ntype = DR_ntype.reshape(batch_size, natoms[ntype], -1)\\r\\n\'DP.calculate_Ei\' is being compiled since it was called from \'DP.forward\'\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 329\\r\\n        emb_list, type_nums = self.get_train_2body_type(atom_type[0])\\r\\n\\r\\n        Ei = self.calculate_Ei(Ri, natoms, batch_size, emb_list, type_nums)\\r\\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\\r\\n```\\r\\n\\r\\n\u62a5\u9519\u4ee3\u7801\u51fa\u73b0\u5728 `tmp_b = torch.matmul(tmp_a, G)` \u8fd9\u884c\u4ee3\u7801\u4e0a\u3002\u8fd9\u662f\u56e0\u4e3a TorchScript \u65e0\u6cd5\u63a8\u65ad `G` \u4e0d\u662f `None`\uff0c\u56e0\u6b64\u5b83\u5c06 `G` \u7684\u7c7b\u578b\u63a8\u65ad\u4e3a `Optional[torch.Tensor]`\u3002\u56e0\u6b64\uff0c\u5f53\u5c06 `G` \u4f20\u9012\u7ed9 `torch.matmul` \u65f6\uff0cTorchScript \u4f1a\u62a5\u9519\uff0c\u56e0\u4e3a `torch.matmul` \u671f\u671b\u4e00\u4e2a `Tensor`\uff0c\u800c\u4e0d\u662f `Optional[torch.Tensor]`\u3002\\r\\n\\r\\n\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u5fc5\u987b\u786e\u4fdd`G`\u5728\u4f20\u9012\u7ed9 `torch.matmul` \u65f6\u4e0d\u662f `None` \u3002\u800c`G`\u901a\u8fc7\u521d\u59cb\u5316\u540e\u5728\u968f\u540e\u4ee3\u7801\u4e2d\u7ecf\u8fc7\u4e86\u8d4b\u503c\uff0c\u53ef\u4ee5\u786e\u4fdd\u5b83\u4e0d\u662f `None`\u3002\u56e0\u6b64\uff0c\u53ef\u4ee5\u4f7f\u7528 `assert` \u8bed\u53e5\u6765\u786e\u4fdd `G` \u4e0d\u662f `None`\u3002\\r\\n\\r\\n```python\\r\\n# ... \u5176\u4ed6\u4ee3\u7801 ...\\r\\nassert G is not None\\r\\ntmp_b = torch.matmul(tmp_a, G)\\r\\n```\\r\\n\\r\\n\u6216\u8005\uff0c\u53ef\u4ee5\u4f7f\u7528 `Optional` \u7c7b\u578b\u7684 `is not None` \u68c0\u67e5\uff1a\\r\\n\\r\\n```python\\r\\n# ... \u5176\u4ed6\u4ee3\u7801 ...\\r\\n# if G is None:\\r\\n#     raise ValueError(\\"G has not been initialized properly.\\")\\r\\nif G is not None:\\r\\n    tmp_b = torch.matmul(tmp_a, G)\\r\\n```\\r\\n\\r\\n---\\r\\n\\r\\n\u7ee7\u7eed\u6267\u884c\uff0c\u53d1\u73b0\u4ee3\u7801\u53c8\u62a5\u9519\u4e86\uff1a\\r\\n\\r\\n```python\\r\\nRuntimeError:\\r\\nundefined value ntype:\\r\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_dp.py\\", line 189\\r\\n            xyz_scater_b = xyz_scater_a[:, :, :, :self.M2]\\r\\n            DR_ntype = torch.matmul(xyz_scater_a.transpose(-2, -1), xyz_scater_b)\\r\\n            DR_ntype = DR_ntype.reshape(batch_size, natoms[ntype], -1)\\r\\n                                                           ~~~~~ <--- HERE\\r\\n\\r\\n            Ei_ntype = self.fitting_net[ntype](DR_ntype)\\r\\n\\r\\n```\\r\\n\\r\\n\u4ee3\u7801\u793a\u4f8b\u5982\u4e0b\uff1a\\r\\n\\r\\n```python\\r\\nEi = None\\r\\nfor type_emb in emb_list:\\r\\n    xyz_scater_a : Optional[torch.Tensor] = None\\r\\n    for emb in type_emb:\\r\\n        ntype, ntype_1 = emb\\r\\n        S_Rij = Ri[:, atom_sum:atom_sum+natoms[ntype], ntype_1 * self.maxNeighborNum:(ntype_1+1) * self.maxNeighborNum, 0].unsqueeze(-1)\\r\\n        embedding_index = ntype * self.ntypes + ntype_1\\r\\n        G: Optional[torch.Tensor] = None\\r\\n        found = False\\r\\n        for idx, emb_net in enumerate(self.embedding_net):\\r\\n            if idx == embedding_index and not found:\\r\\n                G = emb_net(S_Rij)\\r\\n                found = True\\r\\n\\r\\n        tmp_a = Ri[:, atom_sum:atom_sum+natoms[ntype], ntype_1 * self.maxNeighborNum:(ntype_1+1) * self.maxNeighborNum].transpose(-2, -1)\\r\\n        tmp_b = torch.matmul(tmp_a, G)\\r\\n        xyz_scater_a = tmp_b if xyz_scater_a is None else xyz_scater_a + tmp_b\\r\\n\\r\\n    xyz_scater_a = xyz_scater_a / (self.maxNeighborNum * type_nums)\\r\\n    xyz_scater_b = xyz_scater_a[:, :, :, :self.M2]\\r\\n    DR_ntype = torch.matmul(xyz_scater_a.transpose(-2, -1), xyz_scater_b)\\r\\n    DR_ntype = DR_ntype.reshape(batch_size, natoms[ntype], -1)\\r\\n\\r\\n    Ei_ntype: Optional[torch.Tensor] = None\\r\\n    found = False\\r\\n    for idx, fit_net in enumerate(self.fitting_net):\\r\\n        if idx == ntype and not found:\\r\\n            Ei_ntype = fit_net(DR_ntype)\\r\\n            found = True\\r\\n    Ei = Ei_ntype if Ei is None else torch.concat((Ei, Ei_ntype), dim=1)\\r\\n    atom_sum = atom_sum + natoms[ntype]\\r\\n```\\r\\n\\r\\n\u9519\u8bef\u4fe1\u606f\u8868\u660e\u5728\u4f7f\u7528\u53d8\u91cf `ntype` \u65f6\uff0c\u5b83\u7684\u503c\u662f\u672a\u5b9a\u4e49\u7684\u3002\u8fd9\u53ef\u80fd\u662f\u7531\u4e8e\u5728\u4f60\u7684\u4ee3\u7801\u7684\u67d0\u4e2a\u90e8\u5206\uff0c\u7279\u522b\u662f\u5728\u5faa\u73af\u6216\u6761\u4ef6\u8bed\u53e5\u4e4b\u5916\u4f7f\u7528\u4e86 `ntype`\uff0c\u800c\u5728\u8fd9\u4e9b\u60c5\u51b5\u4e0b\uff0c`ntype` \u7684\u503c\u53ef\u80fd\u5c1a\u672a\u88ab\u5206\u914d\u3002\\r\\n\\r\\n\u9519\u8bef\u53d1\u751f\u5728\u4ee5\u4e0b\u884c\uff1a\\r\\n\\r\\n```python\\r\\nDR_ntype = DR_ntype.reshape(batch_size, natoms[ntype], -1)\\r\\n```\\r\\n\\r\\n\u6b64\u5904 `ntype` \u5e94\u8be5\u662f\u5b9a\u4e49\u597d\u7684\uff0c\u56e0\u4e3a\u5b83\u662f\u5728\u524d\u9762\u7684\u5faa\u73af\u4e2d\u5b9a\u4e49\u7684\uff1a\\r\\n\\r\\n```python\\r\\nfor emb in type_emb:\\r\\n    ntype, ntype_1 = emb\\r\\n    ...\\r\\n\\r\\n```\\r\\n\\r\\n\u7531\u4e8e TorchScript \u9700\u8981\u6240\u6709\u53d8\u91cf\u5728\u5b83\u4eec\u88ab\u4f7f\u7528\u4e4b\u524d\u90fd\u6709\u660e\u786e\u7684\u5b9a\u4e49\uff0c\u8fd9\u79cd\u9519\u8bef\u901a\u5e38\u53d1\u751f\u5728\u53d8\u91cf\u53ef\u80fd\u672a\u88ab\u521d\u59cb\u5316\u7684\u60c5\u51b5,\u662f TorchScript \u7f16\u8bd1\u5668\u65e0\u6cd5\u6b63\u786e\u63a8\u65ad `ntype` \u7684\u503c\u3002\u4e00\u4e2a\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u662f\u786e\u4fdd `ntype` \u5728\u5faa\u73af\u5916\u6709\u4e00\u4e2a\u9ed8\u8ba4\u503c\u3002\\r\\n\\r\\n```python\\r\\nntype = 0  # \u6216\u4efb\u4f55\u9002\u5f53\u7684\u9ed8\u8ba4\u503c\\r\\nfor emb in type_emb:\\r\\n    ntype, ntype_1 = emb\\r\\n    ...\\r\\n```\\r\\n\\r\\n\u5982\u679c `type_emb` \u6709\u53ef\u80fd\u4e3a\u7a7a\uff0c\u8fd9\u5c06\u786e\u4fdd `ntype` \u603b\u662f\u6709\u4e00\u4e2a\u5b9a\u4e49\u597d\u7684\u503c\u3002\u8bf7\u6ce8\u610f\uff0c\u8fd9\u53ea\u662f\u4e00\u4e2a\u901a\u7528\u89e3\u51b3\u65b9\u6848\uff0c\u9700\u8981\u6839\u636e\u4f60\u7684\u5177\u4f53\u60c5\u51b5\u6765\u51b3\u5b9a\u6700\u5408\u9002\u7684\u9ed8\u8ba4\u503c\u3002"},{"id":"libtorch1","metadata":{"permalink":"/en/blog/libtorch1","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-10-31-pytorch/2023-10-31-libtorch.md","source":"@site/blog/2023-10-31-pytorch/2023-10-31-libtorch.md","title":"libtorch\u4fee\u6539\u4f7f\u7528\u8bb0\u5f55","description":"\u4f7f\u7528 libtorch \u7684\u65f6\u5019\uff0c\u53d1\u73b0\u6709\u4e9b\u5730\u65b9\u548c pytorch \u7684\u4f7f\u7528\u65b9\u5f0f\u4e0d\u4e00\u6837\uff0c\u8bb0\u5f55\u4e00\u4e0b\u3002","date":"2023-10-31T00:00:00.000Z","formattedDate":"October 31, 2023","tags":[{"label":"python","permalink":"/en/blog/tags/python"},{"label":"libtorch","permalink":"/en/blog/tags/libtorch"}],"readingTime":7.955,"hasTruncateMarker":false,"authors":[{"name":"\u55b5s\u01ceng~","title":"\u4e00\u4ecb\u8349\u6c11","url":"https://github.com/yeahooool","imageURL":"https://z1.ax1x.com/2023/10/31/pinJO8H.jpg","key":"hf"}],"frontMatter":{"slug":"libtorch1","title":"libtorch\u4fee\u6539\u4f7f\u7528\u8bb0\u5f55","authors":["hf"],"tags":["python","libtorch"]},"prevItem":{"title":"libtorch\u4fee\u6539\u4f7f\u7528\u8bb0\u5f552","permalink":"/en/blog/libtorch2"},"nextItem":{"title":"Long Blog Post","permalink":"/en/blog/long-blog-post"}},"content":"\u4f7f\u7528 libtorch \u7684\u65f6\u5019\uff0c\u53d1\u73b0\u6709\u4e9b\u5730\u65b9\u548c pytorch \u7684\u4f7f\u7528\u65b9\u5f0f\u4e0d\u4e00\u6837\uff0c\u8bb0\u5f55\u4e00\u4e0b\u3002\\n\\nTorchScript \u6709\u4e00\u4e9b\u9650\u5236\uff0c\u7279\u522b\u662f\u5173\u4e8e\u5b83\u5982\u4f55\u5904\u7406\u548c\u63a8\u65ad\u7c7b\u578b\u7684\u3002\u6bd4\u5982\u8bf4\u4ee5\u4e0b\u51fd\u6570\u4f20\u5165\u7684 `cfg` \u5b57\u5178\u5305\u542b\u4e86\u4e0d\u540c\u7c7b\u578b\u7684\u503c\uff08`List[int]`\u3001`bool` \u548c `str`\uff09\uff0c\u8fd9\u4f7f\u5f97 TorchScript \u96be\u4ee5\u5904\u7406\u3002\\n\\n```python\\nclass EmbeddingNet(nn.Module):\\n    def __init__(self, cfg, magic=False):\\n        super(EmbeddingNet, self).__init__()\\n        self.cfg = cfg\\n        self.weights = nn.ParameterDict()\\n\\n        if cfg[\'bias\']:\\n            self.bias = nn.ParameterDict()\\n        if self.cfg[\'resnet_dt\']:\\n                self.resnet_dt = nn.ParameterDict()\\n        self.network_size = [1] + self.cfg[\'network_size\']\\n        if cfg[\\"activation\\"] == \\"tanh\\":\\n            cfg[\\"activation\\"] = torch.tanh\\n        else:\\n            pass\\n```\\n\\n\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u53ef\u4ee5\u5728 `EmbeddingNet` \u7c7b\u4e2d\u4f7f\u7528\u660e\u786e\u7684\u7c7b\u5c5e\u6027\u4ee3\u66ff `cfg` \u5b57\u5178\uff0c\u5e76\u4e3a\u8fd9\u4e9b\u5c5e\u6027\u63d0\u4f9b\u6b63\u786e\u7684\u7c7b\u578b\u6ce8\u91ca\u3002\u8fd9\u6709\u52a9\u4e8e TorchScript \u63a8\u65ad\u8fd9\u4e9b\u5c5e\u6027\u7684\u7c7b\u578b\uff0c\u5e76\u4f7f\u4ee3\u7801\u4e0e TorchScript \u66f4\u52a0\u517c\u5bb9\u3002\u4ee5\u4e0b\u662f\u6839\u636e\u8fd9\u79cd\u7b56\u7565\u4fee\u6539\u8fc7\u7684 `EmbeddingNet` \u7c7b\uff1a\\n\\n```python\\nclass EmbeddingNet(nn.Module):\\n    def __init__(self,\\n                 network_size: List[int],\\n                 bias: bool,\\n                 resnet_dt: bool,\\n                 activation: str,\\n                 device,\\n                 magic = False):\\n        super(EmbeddingNet, self).__init__()\\n        self.network_size = [1] + network_size\\n        self.bias_flag = bias\\n        self.resnet_dt_flag = resnet_dt\\n        self.activation = torch.tanh if activation == \\"tanh\\" else None\\n\\n        self.weights = nn.ParameterDict()\\n        self.bias_dict = nn.ParameterDict() if bias else None\\n        self.resnet_dt_dict = nn.ParameterDict() if resnet_dt else None\\n```\\n\\n\u8fd9\u6837\u505a\u5176\u5b9e\u5c31\u662f\u5728\u8c03\u7528 `EmbeddingNet` \u7684\u65f6\u5019\uff0c\u628a `cfg` \u5b57\u5178\u4e2d\u7684\u503c\u4e00\u4e2a\u4e2a\u660e\u786e\u5730\u4f20\u5165 `EmbeddingNet` \u7c7b\u4e2d\uff0c\u8fd9\u6837 TorchScript \u5c31\u80fd\u591f\u63a8\u65ad\u51fa\u8fd9\u4e9b\u503c\u7684\u7c7b\u578b\u4e86\u3002\\n\\n```python\\nEmbeddingNet(self.config[\\"net_cfg\\"][\\"embedding_net\\"][\\"network_size\\"],\\n             self.config[\\"net_cfg\\"][\\"embedding_net\\"][\\"bias\\"],\\n             self.config[\\"net_cfg\\"][\\"embedding_net\\"][\\"resnet_dt\\"],\\n             self.config[\\"net_cfg\\"][\\"embedding_net\\"][\\"activation\\"],\\n             self.device,\\n             magic)\\n```\\n\\n---\\n\\n\u540e\u7eed\u7a0b\u5e8f\u8fd0\u884c\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u62a5\u9519\u4e86\uff0c\u62a5\u9519\u4fe1\u606f\u5982\u4e0b\uff1a\\n\\n```python\\nRuntimeError:\\nUnable to extract string literal index. ModuleDict indexing is only supported with string literals. For example, \'i = \\"a\\"; self.layers[i](x)\' will fail because i is not a literal. Enumeration of ModuleDict is supported, e.g. \'for k, v in self.items(): out = v(inp)\':\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_embedding.py\\", line 89\\n\\n            if self.bias_flag:\\n                hiden = torch.matmul(x, self.weights[\'weight\' + str(i-1)]) + self.bias[\'bias\' + str(i-1)]\\n                                        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\\n            else:\\n                hiden = torch.matmul(x, self.weights[\'weight\' + str(i-1)])\\n```\\n\\n\u8fd9\u4e2a\u65b0\u7684\u9519\u8bef\u8868\u660e TorchScript \u5728\u5904\u7406\u5b57\u5178\u952e\u65f6\u9047\u5230\u4e86\u95ee\u9898\u3002TorchScript \u9700\u8981\u5728\u7f16\u8bd1\u65f6\u77e5\u9053\u786e\u5207\u7684\u952e\uff0c\u4f46\u5728\u4ee3\u7801\u4e2d\uff0c\u952e\u662f\u901a\u8fc7\u5b57\u7b26\u4e32\u8fde\u63a5\u52a8\u6001\u751f\u6210\u7684\uff0c\u8fd9\u4f7f\u5f97 TorchScript \u65e0\u6cd5\u5728\u7f16\u8bd1\u65f6\u786e\u5b9a\u952e\u7684\u503c\u3002\\n\\n\u7531\u4e8e `nn.ParameterDict` \u672c\u8d28\u4e0a\u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u5e76\u4e14\u5728 forward \u51fd\u6570\u4e2d\u4f7f\u7528\u7684\u952e\u662f\u52a8\u6001\u751f\u6210\u7684\uff0cTorchScript \u65e0\u6cd5\u5728\u7f16\u8bd1\u65f6\u89e3\u6790\u8fd9\u4e9b\u952e\u3002\u4e3a\u4e86\u514b\u670d\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u53ef\u4ee5\u5c1d\u8bd5\u4e0d\u4f7f\u7528 `nn.ParameterDict`\uff0c\u800c\u662f\u76f4\u63a5\u4f7f\u7528 `nn.Parameter` \u5217\u8868\u6216 `nn.ModuleList`\\n\\n```python\\nself.weights = nn.ParameterDict()\\nself.bias_dict = nn.ParameterDict() if bias else None\\nself.resnet_dt_dict = nn.ParameterDict() if resnet_dt else None\\n# initial weight normalization\\nfor i in range(1, len(self.network_size)):\\n    wij = torch.Tensor(self.network_size[i-1], self.network_size[i])\\n    normal(wij, mean=0, std=(1.0 / np.sqrt(self.network_size[i-1] + self.network_size[i])))\\n    self.weights[\\"weight\\" + str(i-1)] = nn.Parameter(wij, requires_grad=True)\\n    if self.bias_flag:\\n        bias = torch.Tensor(1, self.network_size[i])\\n        normal(bias, mean=0, std=1)\\n        self.bias[\\"bias\\" + str(i-1)] = nn.Parameter(bias, requires_grad=True)\\n    if self.resnet_dt_flag:\\n        resnet_dt = torch.Tensor(1, self.network_size[i])\\n        normal(resnet_dt, mean=1, std=0.001)\\n        self.resnet_dt[\\"resnet_dt\\" + str(i-1)] = nn.Parameter(resnet_dt, requires_grad=True)\\n\\ndef forward(self, x):\\n    for i in range(1, len(self.network_size)):\\n        if self.bias_flag:\\n            hiden = torch.matmul(x, self.weights[\'weight\' + str(i-1)]) + self.bias[\'bias\' + str(i-1)]\\n        else:\\n            hiden = torch.matmul(x, self.weights[\'weight\' + str(i-1)])\\n\\n        hiden = self.activation(hiden)\\n\\n        if self.network_size[i] == self.network_size[i-1]:\\n            if self.resnet_dt_flag:\\n                x = hiden * self.resnet_dt[\'resnet_dt\' + str(i-1)] + x\\n            else:\\n                x = hiden + x\\n        elif self.network_size[i] == 2 * self.network_size[i-1]:\\n            if self.resnet_dt_flag:\\n                x = torch.cat((x, x), dim=-1)  + hiden * self.resnet_dt[\'resnet_dt\' + str(i-1)]\\n            else:\\n                x = torch.cat((x, x), dim=-1)  + hiden\\n        else:\\n            x = hiden\\n    return x\\n```\\n\\n\u4ee5\u4e0b\u662f\u4e00\u4e2a\u4f7f\u7528 `nn.Parameter` \u5217\u8868\u7684\u793a\u4f8b\uff0c\u5b83\u907f\u514d\u4e86\u5728 forward \u65b9\u6cd5\u4e2d\u4f7f\u7528\u52a8\u6001\u952e\uff1a\\n\\n```python\\n# \u521d\u59cb\u5316\u6743\u91cd\u548c\u504f\u5dee\u5217\u8868\\nself.weights = nn.ParameterList()\\nself.bias = nn.ParameterList() if bias else None\\nself.resnet_dt = nn.ParameterList() if resnet_dt else None\\n\\n# \u521d\u59cb\u5316\u6743\u91cd\u548c\u504f\u5dee\\nfor i in range(1, len(self.network_size)):\\n    wij = torch.Tensor(self.network_size[i-1], self.network_size[i])\\n    normal(wij, mean=0, std=(1.0 / np.sqrt(self.network_size[i-1] + self.network_size[i])))\\n    self.weights.append(nn.Parameter(wij, requires_grad=True)).to(\'cuda:2\')\\n\\n    if self.bias_flag:\\n        bias_tensor = torch.Tensor(1, self.network_size[i])\\n        normal(bias_tensor, mean=0, std=1)\\n        self.bias.append(nn.Parameter(bias_tensor, requires_grad=True)).to(\'cuda:2\')\\n\\n    if self.resnet_dt_flag:\\n        resnet_dt = torch.Tensor(1, self.network_size[i])\\n        normal(resnet_dt, mean=1, std=0.001)\\n        self.resnet_dt.append(nn.Parameter(resnet_dt, requires_grad=True)).to(\'cuda:2\')\\n\\ndef forward(self, x):\\n    for i in range(1, len(self.network_size)):\\n        weight = self.weights[i-1]\\n        if self.bias_flag:\\n            bias = self.bias[i-1]\\n            hiden = torch.matmul(x, weight) + bias\\n        else:\\n            hiden = torch.matmul(x, weight)\\n\\n        hiden = self.activation_function(hiden)\\n\\n        if self.network_size[i] == self.network_size[i-1]:\\n            if self.resnet_dt_flag:\\n                resnet_dt = elf.resnet_dt[i-1]\\n                x = hiden * resnet_dt + x\\n            else:\\n                x = hiden + x\\n        elif self.network_size[i] == 2 * self.network_size[i-1]:\\n            if self.resnet_dt_flag:\\n                resnet_dt = elf.resnet_dt[i-1]\\n                x = torch.cat((x, x), dim=-1) + hiden * resnet_dt\\n            else:\\n                x = torch.cat((x, x), dim=-1) + hiden\\n        else:\\n            x = hiden\\n\\n    return x\\n```\\n\\n---\\n\\n\u540e\u7eed\u7a0b\u5e8f\u8fd0\u884c\u7684\u65f6\u5019\uff0c\u53d1\u73b0\u4ecd\u7136\u62a5\u9519\u4e86\uff0c\u62a5\u9519\u4fe1\u606f\u5982\u4e0b\uff1a\\n\\n```python\\nRuntimeError:\\nExpected integer literal for index but got a variable or non-integer. ModuleList/Sequential indexing is only supported with integer literals. For example, \'i = 4; self.layers[i](x)\' will fail because i is not a literal. Enumeration is supported, e.g. \'for index, v in enumerate(self): out = v(inp)\':\\n  File \\"/data/home/hfhuang/software/PWmatMLFF/src/model/dp_embedding.py\\", line 93\\n\\n            weight = self.weights[i-1]\\n                     ~~~~~~~~~~~~~~~~~ <--- HERE\\n            if self.bias_flag:\\n                bias = self.bias[i-1]\\n```\\n\\n\u8fd9\u4e2a\u9519\u8bef\u4fe1\u606f\u518d\u6b21\u7a81\u51fa\u4e86 TorchScript \u7f16\u8bd1\u8fc7\u7a0b\u4e2d\u7684\u4e00\u4e9b\u9650\u5236\u3002TorchScript \u671f\u671b\u5728\u7f16\u8bd1\u65f6\u80fd\u591f\u77e5\u9053\u6240\u6709\u7684\u7d22\u5f15\uff0c\u4f46\u5728\u4ee3\u7801\u4e2d\uff0c\u8bd5\u56fe\u4f7f\u7528\u53d8\u91cf `i` \u6765\u7d22\u5f15 `nn.ParameterList`\uff0c\u8fd9\u662f TorchScript \u4e0d\u652f\u6301\u7684\u3002\\n\\n\u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u53ef\u4ee5\u7528\u4e00\u4e2a\u5c0f\u6280\u5de7\uff1a\u4f7f\u7528 Python \u5185\u7f6e\u7684 `enumerate` \u51fd\u6570\u6765\u904d\u5386\u6743\u91cd\u548c\u504f\u7f6e\uff0c\u8fd9\u6837\u6211\u4eec\u53ef\u4ee5\u540c\u65f6\u83b7\u5f97\u5f53\u524d\u7684\u7d22\u5f15\u548c\u5bf9\u5e94\u7684\u53c2\u6570\u3002\\n\\n\u56e0\u4e3a\u6709\u591a\u4e2a\u9700\u8981\u904d\u5386\u7684\u53c2\u6570\u5217\u8868\uff08`self.weights`, `self.bias`, `self.resnet_dt`\uff09\uff0c\u800c TorchScript \u4e0d\u5141\u8bb8\u4f7f\u7528\u975e\u6587\u5b57\u7d22\u5f15\uff08\u5373\u53d8\u91cf\u7d22\u5f15\uff09\u3002\\n\\n\u90a3\u4e48\u4e00\u79cd\u53ef\u80fd\u7684\u89e3\u51b3\u65b9\u6848\u662f\u5728\u521d\u59cb\u5316\u65f6\u5c06\u8fd9\u4e9b\u6743\u91cd\u3001\u504f\u7f6e\u548c resnet_dt \u53c2\u6570\u7ec4\u5408\u6210\u4e00\u4e2a\u5355\u72ec\u7684\u6a21\u5757\u5217\u8868 (`nn.ModuleList`)\uff0c\u6bcf\u4e2a\u6a21\u5757\u90fd\u6709\u5b83\u9700\u8981\u7684\u53c2\u6570\u3002\u8fd9\u6837\uff0c\u53ef\u4ee5\u4e00\u6b21\u904d\u5386\u4e00\u4e2a\u6a21\u5757\uff0c\u5e76\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u5e94\u7528\u903b\u8f91\u3002\\n\\n\u4e0b\u9762\u662f\u5982\u4f55\u505a\u5230\u8fd9\u4e00\u70b9\u7684\u793a\u4f8b\u4ee3\u7801\uff1a\\n\\n```python\\nclass LayerModule(nn.Module):\\n    def __init__(self,\\n                 weight,\\n                 bias = None,\\n                 resnet_dt = None):\\n        super(LayerModule, self).__init__()\\n        self.weight = nn.Parameter(weight, requires_grad=True)\\n        self.bias = nn.Parameter(bias, requires_grad=True) if bias is not None else None\\n        self.resnet_dt = nn.Parameter(resnet_dt, requires_grad=True) if resnet_dt is not None else None\\n\\nclass EmbeddingNet(nn.Module):\\n    def __init__(self,\\n                 network_size: List[int],\\n                 bias: bool,\\n                 resnet_dt: bool,\\n                 activation: str,\\n                 device,\\n                 magic = False):\\n        super(EmbeddingNet, self).__init__()\\n        self.network_size = [1] + network_size\\n        self.bias_flag = bias\\n        self.resnet_dt_flag = resnet_dt\\n        self.activation = torch.tanh if activation == \\"tanh\\" else None\\n        # \u521b\u5efa\u6a21\u5757\u5217\u8868\\n        self.layers = nn.ModuleList()\\n\\n        # initial weight normalization\\n        for i in range(1, len(self.network_size)):\\n            wij = torch.Tensor(self.network_size[i-1], self.network_size[i])\\n            normal(wij, mean=0, std=(1.0 / np.sqrt(self.network_size[i-1] + self.network_size[i])))\\n\\n            bias = None\\n            if self.bias_flag:\\n                bias = torch.Tensor(1, self.network_size[i])\\n                normal(bias, mean=0, std=1)\\n\\n            resnet_dt = None\\n            if self.resnet_dt_flag:\\n                resnet_dt = torch.Tensor(1, self.network_size[i])\\n                normal(resnet_dt, mean=1, std=0.001)\\n            # \u6dfb\u52a0\u5230\u6a21\u5757\u5217\u8868\\n            self.layers.append(LayerModule(wij, bias, resnet_dt)).to(device)\\n\\n    def forward(self, x):\\n        for i, layer in enumerate(self.layers):\\n            if self.bias_flag and layer.bias is not None:\\n                hiden = torch.matmul(x, layer.weight) + layer.bias\\n            else:\\n                hiden = torch.matmul(x, layer.weight)\\n\\n            hiden = self.activation(hiden)\\n\\n            if self.network_size[i+1] == self.network_size[i]:\\n                if self.resnet_dt_flag and layer.resnet_dt is not None:\\n                    x = hiden * layer.resnet_dt + x\\n                else:\\n                    x = hiden + x\\n            elif self.network_size[i+1] == 2 * self.network_size[i]:\\n                if self.resnet_dt_flag and layer.resnet_dt is not None:\\n                    x = torch.cat((x, x), dim=-1)  + hiden * layer.resnet_dt\\n                else:\\n                    x = torch.cat((x, x), dim=-1)  + hiden\\n            else:\\n                x = hiden\\n\\n        return x\\n```\\n\\n\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff0c\u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a\u65b0\u7684 `LayerModule` \u7c7b\uff0c\u5b83\u5305\u542b\u4e00\u4e2a\u5c42\u7684`weight`\u3001`bias`\u548c `resnet_dt`\uff08\u5982\u679c\u5b58\u5728\u7684\u8bdd\uff09\u3002\u7136\u540e\uff0c`EmbeddingNet` \u7c7b\u521b\u5efa\u4e86\u4e00\u4e2a `nn.ModuleList`\uff0c\u5176\u4e2d\u5305\u542b\u4e86\u6240\u6709\u7684 `LayerModule` \u5b9e\u4f8b\u3002\u5728 `forward` \u65b9\u6cd5\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u65b9\u4fbf\u5730\u904d\u5386 `self.layers`\uff0c\u5e76\u5bf9\u6bcf\u4e2a `LayerModule` \u5b9e\u4f8b\u5e94\u7528\u6211\u4eec\u7684\u903b\u8f91\u3002\\n\\n\u8fd9\u79cd\u65b9\u6cd5\u7684\u597d\u5904\u662f\uff0c\u5b83\u5141\u8bb8 TorchScript \u5728\u7f16\u8bd1\u65f6\u77e5\u9053\u6240\u6709\u7684\u7d22\u5f15\u548c\u7ed3\u6784\uff0c\u56e0\u4e3a\u6211\u4eec\u4e0d\u518d\u4f9d\u8d56\u4e8e\u52a8\u6001\u7684\u7d22\u5f15\u6216\u952e\u6765\u8bbf\u95ee\u6211\u4eec\u7684\u53c2\u6570\u3002"},{"id":"long-blog-post","metadata":{"permalink":"/en/blog/long-blog-post","editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2023-05-29-long-blog-post.md","source":"@site/blog/2023-05-29-long-blog-post.md","title":"Long Blog Post","description":"This is the summary of a very long blog post,","date":"2023-05-29T00:00:00.000Z","formattedDate":"May 29, 2023","tags":[{"label":"hello","permalink":"/en/blog/tags/hello"},{"label":"docusaurus","permalink":"/en/blog/tags/docusaurus"}],"readingTime":2.05,"hasTruncateMarker":true,"authors":[{"name":"\u55b5s\u01ceng~","title":"\u4e00\u4ecb\u8349\u6c11","url":"https://github.com/yeahooool","imageURL":"https://z1.ax1x.com/2023/10/31/pinJO8H.jpg","key":"hf"}],"frontMatter":{"slug":"long-blog-post","title":"Long Blog Post","authors":"hf","tags":["hello","docusaurus"]},"prevItem":{"title":"libtorch\u4fee\u6539\u4f7f\u7528\u8bb0\u5f55","permalink":"/en/blog/libtorch1"}},"content":"This is the summary of a very long blog post,\\n\\nUse a `\x3c!--` `truncate` `--\x3e` comment to limit blog post size in the list view.\\n\\n\x3c!--truncate--\x3e\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet\\n\\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Pellentesque elementum dignissim ultricies. Fusce rhoncus ipsum tempor eros aliquam consequat. Lorem ipsum dolor sit amet"}]}')}}]);